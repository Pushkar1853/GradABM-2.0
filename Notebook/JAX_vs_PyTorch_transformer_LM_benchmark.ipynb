{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EuOcMJmZw1jN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enolan/jax-torch-bench/blob/master/JAX_vs_PyTorch_transformer_LM_benchmark.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DLbS-ISEN_W",
        "outputId": "a09d6f0d-e89e-483d-c6fb-3f6ea82470e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 21 13:53:07 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Config is tuned for a P100, a 16GB GPU. You'll have to reduce the batch sizes to get\n",
        "# the models running if Colab gives you something smaller.\n",
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GnyCohiXCm3_"
      },
      "source": [
        "# Common setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "He4JoFYEptcu",
        "outputId": "3d12d198-cd1e-4c87-8ec5-d5c53472e425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-21 13:53:07--  http://mattmahoney.net/dc/enwik9.zip\n",
            "Resolving mattmahoney.net (mattmahoney.net)... 34.198.1.81\n",
            "Connecting to mattmahoney.net (mattmahoney.net)|34.198.1.81|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322592222 (308M) [application/zip]\n",
            "Saving to: ‘enwik9.zip.2’\n",
            "\n",
            "enwik9.zip.2        100%[===================>] 307.65M  31.8MB/s    in 10s     \n",
            "\n",
            "2023-05-21 13:53:18 (29.8 MB/s) - ‘enwik9.zip.2’ saved [322592222/322592222]\n",
            "\n",
            "Archive:  enwik9.zip\n",
            "replace enwik9? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: enwik9                  y\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://mattmahoney.net/dc/enwik9.zip\n",
        "!unzip enwik9.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "naYZSB2yqHoZ"
      },
      "outputs": [],
      "source": [
        "!pip install optax flax -q\n",
        "!pip install -U numpy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKDj1z15m81t",
        "outputId": "7fc43528-9b05-44c5-9372-9a9b6954c47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'jax-torch-bench' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/enolan/jax-torch-bench.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWHnhLyWL-_i",
        "outputId": "013ed63d-7b7a-4193-a67e-65ea55680b65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/jax-torch-bench\n"
          ]
        }
      ],
      "source": [
        "%cd jax-torch-bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0uLr90ovCupj"
      },
      "outputs": [],
      "source": [
        "from config import ModelConfig\n",
        "\n",
        "enwik9 = \"/content/enwik9\"\n",
        "\n",
        "# This is the highest batch size PyTorch can handle, the JAX model can do 79\n",
        "cfg = ModelConfig(seq_len=256, n_layers=12, d_model=512, num_heads=8, ff_dim=3072, dropout=0.1, batch_size=63, learning_rate=1e-3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8zV7tab1Ch4z"
      },
      "source": [
        "# JAX model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lHrSyChPkVBd"
      },
      "outputs": [],
      "source": [
        "# !pip install jax_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yfYJzTMXqpdm"
      },
      "outputs": [],
      "source": [
        "from jax_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mms4hEafx7PR"
      },
      "outputs": [],
      "source": [
        "# Set up the model\n",
        "params, model, optimizer, opt_state, sample = setup_all(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6AdAkqQim-c",
        "outputId": "21c4ae56-11af-4285-8b43-0f3a3d8698b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "# This will run the training loop indefinitely. Hit the stop button to abort.\n",
        "params, opt_state = train_loop(model, optimizer, opt_state, params, cfg, enwik9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwY1-5tehAK9"
      },
      "outputs": [],
      "source": [
        "sample(params, \"'''Star Trek'''\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "99j1fHUFDr_O"
      },
      "source": [
        "# PyTorch model\n",
        "\n",
        "Since JAX preallocates all GPU memory, you'll need to restart the runtime (Runtime -> Restart runtime) to try the PyTorch model. Then rerun the config setup cell before running the ones below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B0iUAGmwR39M"
      },
      "outputs": [],
      "source": [
        "from pytorch_model import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NBa39lPzR9Wg"
      },
      "outputs": [],
      "source": [
        "lm = LM(cfg).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvoKZykNR_UU"
      },
      "outputs": [],
      "source": [
        "train_loop(lm, cfg, enwik9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBoNK-cISWkB"
      },
      "outputs": [],
      "source": [
        "lm.sample(\"'''Star Trek'''\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CK9ibF5mU-B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JAX vs PyTorch transformer LM benchmark",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
