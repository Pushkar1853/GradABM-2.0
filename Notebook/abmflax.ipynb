{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"!pip install epiweeks -q\n!pip install torch_geometric -q","metadata":{"execution":{"iopub.status.busy":"2023-05-20T10:33:22.535470Z","iopub.execute_input":"2023-05-20T10:33:22.535871Z","iopub.status.idle":"2023-05-20T10:33:44.883384Z","shell.execute_reply.started":"2023-05-20T10:33:22.535831Z","shell.execute_reply":"2023-05-20T10:33:44.882121Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import jax\nimport jax.numpy as np\nfrom jax import random\n# from jax.nn import pad\nfrom epiweeks import Week, Year\nimport pandas as pd\nimport os\nimport torch\ndtype = torch.float\nWEEKS_AHEAD = 4\nPAD_VALUE = -999\nDAYS_IN_WEEK = 7\nNOISE_LEVELS_FLU = [0.15, 0.25, 0.50, 0.75]\nNOISE_LEVELS_COVID = [0.5, 1.0, 1.5, 2.0]\nfrom flax import linen as nn\nfrom sklearn.preprocessing import StandardScaler\nimport pdb\nimport math\nimport pandas as pd\nimport os\nimport yaml\nimport seaborn as sns\nimport flax\nfrom flax import struct\nfrom flax.training import train_state\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import MessagePassing\nfrom abc import ABC, abstractmethod\nfrom scipy.stats import gamma\nimport math\n# from flax import functional as F\nimport yaml\nINITIAL_INFECTED_RATIO = 0.5\nINFINITY_TIME = np.inf\nUSE_SPARSE = False\nimport random\nimport jax.nn as nn\nimport time\nfrom copy import copy\nimport matplotlib.pyplot as plt\nimport pdb\nfrom epiweeks import Week\nimport argparse\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nimport traceback\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state\nfrom jax.nn import *\nfrom tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-20T11:48:41.362157Z","iopub.execute_input":"2023-05-20T11:48:41.363125Z","iopub.status.idle":"2023-05-20T11:48:41.374383Z","shell.execute_reply.started":"2023-05-20T11:48:41.363091Z","shell.execute_reply":"2023-05-20T11:48:41.373355Z"},"trusted":true},"execution_count":158,"outputs":[]},{"cell_type":"markdown","source":"# Data Utils","metadata":{}},{"cell_type":"code","source":"datapath = \"/kaggle/input/gradabmdata/Data/Processed/covid_data.csv\"\ncounty_datapath = f\"/kaggle/input/gradabmdata/Data/Processed/county_data.csv\"\ndatapath_flu_hhs = \"/kaggle/input/gradabmdata/Data/Processed/flu_region_data.csv\"\ndatapath_flu_state = \"/kaggle/input/gradabmdata/Data/Processed/flu_state_data.csv\"\npopulation_path = \"/kaggle/input/gradabmdata/Data/ABM_parameters/table_population.csv\"\nEW_START_DATA = \"202012\"\nEW_START_DATA_FLU = \"201740\"  # for convenience\n\n# Select signals COVID\nmacro_features = [\n    \"retail_and_recreation_percent_change_from_baseline\",\n    \"grocery_and_pharmacy_percent_change_from_baseline\",\n    \"parks_percent_change_from_baseline\",\n    \"transit_stations_percent_change_from_baseline\",\n    \"workplaces_percent_change_from_baseline\",\n    \"residential_percent_change_from_baseline\",\n    \"apple_mobility\",\n    \"death_jhu_incidence\",\n    \"positiveIncr\",\n]\n\n# Select signals Flu\ninclude_cols = [\n    \"symptom:Fever\",\n    \"symptom:Low-grade fever\",\n    \"symptom:Cough\",\n    \"symptom:Sore throat\",\n    \"symptom:Headache\",\n    \"symptom:Fatigue\",\n    \"symptom:Vomiting\",\n    \"symptom:Diarrhea\",\n    \"symptom:Shortness of breath\",\n    \"symptom:Chest pain\",\n    \"symptom:Dizziness\",\n    \"symptom:Confusion\",\n    \"symptom:Generalized tonicâ€“clonic seizure\",\n    \"symptom:Weakness\",\n]\n\nstates = [\n    \"AL\",\n    \"AK\",\n    \"AZ\",\n    \"AR\",\n    \"CA\",\n    \"CO\",\n    \"CT\",\n    \"DE\",\n    \"DC\",\n    \"FL\",\n    \"GA\",\n    \"ID\",\n    \"IL\",\n    \"IN\",\n    \"IA\",\n    \"KS\",\n    \"KY\",\n    \"LA\",\n    \"ME\",\n    \"MD\",\n    \"MA\",\n    \"MI\",\n    \"MN\",\n    \"MS\",\n    \"MO\",\n    \"MT\",\n    \"NE\",\n    \"NV\",\n    \"NH\",\n    \"NJ\",\n    \"NM\",\n    \"NY\",\n    \"NC\",\n    \"ND\",\n    \"OH\",\n    \"OK\",\n    \"OR\",\n    \"PA\",\n    \"RI\",\n    \"SC\",\n    \"SD\",\n    \"TN\",\n    \"TX\",\n    \"UT\",\n    \"VT\",\n    \"VA\",\n    \"WA\",\n    \"WV\",\n    \"WI\",\n    \"WY\",\n    \"X\",\n]\n\ncounties = {\n    \"MA\": [\n        \"25003\",\n#         \"25005\",\n        \"25009\",\n        \"25011\",\n        \"25013\",\n        \"25015\",\n        \"25021\",\n        \"25023\",\n        \"25027\",\n    ]\n}\n\n########################################################\n#           helpers\n########################################################\n\ndef convert_to_epiweek(x):\n    return Week.fromstring(str(x))\n\n\ndef get_epiweeks_list(start_ew, end_ew):\n    \"\"\"\n    Returns a list of epiweeks objects between start_ew and end_ew (inclusive).\n    This is useful for iterating through these weeks.\n    \"\"\"\n    if isinstance(start_ew, str):\n        start_ew = convert_to_epiweek(start_ew)\n    if isinstance(end_ew, str):\n        end_ew = convert_to_epiweek(end_ew)\n    iter_weeks = (\n        list(Year(2017).iterweeks())\n        + list(Year(2018).iterweeks())\n        + list(Year(2019).iterweeks())\n        + list(Year(2020).iterweeks())\n        + list(Year(2021).iterweeks())\n    )\n    idx_start = iter_weeks.index(start_ew)\n    idx_end = iter_weeks.index(end_ew)\n    return iter_weeks[idx_start : idx_end + 1]\n\n\n# def create_window_seqs(X, y, min_sequence_length):\n#     \"\"\"\n#     Creates windows of fixed size with appended zeros.\n#     Args:\n#         X: Features\n#         y: Targets, in synchrony with features (i.e. x[t] and y[t] correspond to the same time)\n#         min_sequence_length: Minimum length of the sequence\n#     \"\"\"\n#     # Convert to small sequences for training, starting with length 10\n#     seqs = []\n#     targets = []\n#     mask_ys = []\n\n#     # Starts at sequence_length and goes until the end\n#     for idx in range(min_sequence_length, X.shape[0] + 1, 1):\n#         # Sequences\n#         seqs.append(np.array(X[:idx, :]))\n#         # Targets\n#         y_ = y[:idx]\n#         mask_y = np.ones(len(y_))\n#         targets.append(np.array(y_))\n#         mask_ys.append(mask_y)\n#     seqs = pad_sequence(seqs, batch_first=True, padding_value=0).type(dtype)\n#     ys = pad_sequence(targets, batch_first=True, padding_value=PAD_VALUE).type(dtype)\n#     mask_ys = pad_sequence(mask_ys, batch_first=True, padding_value=0).type(dtype)\n\n#     return seqs, ys, mask_ys\n\ndef create_window_seqs(seq, ys, min_sequence_length):\n    seqs = []\n    ys_seqs = []\n    mask_seqs = []\n\n    for i in range(len(seq) - min_sequence_length + 1):\n        seqs.append(seq[i:i+min_sequence_length])\n        ys_seqs.append(ys[i:i+min_sequence_length])\n        mask_seqs.append(jnp.ones(min_sequence_length))\n\n    seqs = jnp.array(seqs)\n    ys_seqs = jnp.array(ys_seqs)\n    mask_seqs = jnp.array(mask_seqs)\n\n    return seqs, ys_seqs, mask_seqs\n\n########################################################\n#           COVID: state/national level data\n########################################################\n\ndef load_df(region, ew_start_data, ew_end_data):\n    \"\"\"Load and clean data.\"\"\"\n    df = pd.read_csv(datapath, low_memory=False)\n    df = df[(df[\"region\"] == region)]\n    df[\"epiweek\"] = df.loc[:, \"epiweek\"].apply(convert_to_epiweek)\n    # Subset data using init parameters\n    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n    df = df.fillna(method=\"ffill\")\n    df = df.fillna(method=\"backfill\")\n    df = df.fillna(0)\n    return df\n\n\ndef get_state_train_data(region, pred_week, ew_start_data=EW_START_DATA):\n    \"\"\"Get processed dataframe of data + target as array.\"\"\"\n    # Import data\n    region = str.upper(region)\n    pred_week = convert_to_epiweek(pred_week)\n    ew_start_data = convert_to_epiweek(ew_start_data)\n    df = load_df(region, ew_start_data, pred_week)\n    # Select targets\n    targets = df.loc[:, [\"positiveIncr\"]].values\n    # Now subset based on input ew_start_data\n    df = df[macro_features]\n    return df, targets\n\n\ndef get_state_test_data(region, pred_week):\n    \"\"\"\n    @param pred_week: Prediction week\n    \"\"\"\n    pred_week = convert_to_epiweek(pred_week)\n    # Import smoothed dataframe\n    df = load_df(region, pred_week + 1, pred_week + 4)\n    new_cases = df.loc[:, \"positiveIncr\"].values\n    new_deaths = df.loc[:, \"death_jhu_incidence\"].values\n    return new_cases, new_deaths\n\n\ndef get_train_targets_all_regions(pred_week):\n    deaths_all_regions = {}\n    for region in states:\n        _, targets = get_state_train_data(region, pred_week)\n        deaths_all_regions[region] = targets[:, 0]  # index 0 is inc positive cases\n    return deaths_all_regions\n\n\ndef get_train_features_all_regions(pred_week):\n    features_all_regions = {}\n    for region in states:\n        df, _ = get_state_train_data(region, pred_week)\n        features_all_regions[region] = df.to_numpy()\n    return features_all_regions\n\n########################################################\n#           COVID: county level data\n# note: to obtain data, use get_features_per_county.ipynb\n########################################################\n\ndef load_county_df(county, ew_start_data, ew_end_data):\n    \"\"\"Load and clean data\"\"\"\n    df = pd.read_csv(county_datapath)\n    df = df[(df[\"geo_value\"] == int(county))]\n    from datetime import datetime\n    from datetime import date\n\n    def convert_date_to_epiweek(x):\n        if isinstance(x, Week):\n            return x\n        else:\n            date = datetime.strptime(x, \"%Y-%m-%d\")\n            return Week.fromdate(date)\n\n    df[\"epiweek\"] = df.loc[:, \"time_value\"].apply(convert_date_to_epiweek)\n    # Subset data using init parameters\n    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n    df = df.fillna(0)  # There are zeros at the beginning\n    return df\n\n\ndef get_county_train_data(\n    county, pred_week, ew_start_data=EW_START_DATA, noise_level=0\n):\n    \"\"\"Get processed dataframe of data + target as array\"\"\"\n    # Import data\n    pred_week = convert_to_epiweek(pred_week)\n    ew_start_data = convert_to_epiweek(ew_start_data)\n    df = load_county_df(county, ew_start_data, pred_week)\n    # Select targets\n    targets = df.loc[:, [\"cases\", \"deaths\"]].values\n    if noise_level > 0:\n        # noise_level is an index for your list\n        noise = NOISE_LEVELS_COVID[noise_level - 1]\n        std_vals = np.std(targets, axis=0) * noise\n        noise_dist = np.random.normal(scale=std_vals, size=targets.shape)\n        noisy_targets = targets + noise_dist\n        noisy_targets = noisy_targets.astype(\"int32\")\n        targets = np.maximum(noisy_targets, 0)\n    df.drop(columns=[\"epiweek\", \"geo_value\", \"time_value\"], inplace=True)\n    return df, targets\n\n\ndef get_county_test_data(county, pred_week):\n    \"\"\"\n    @param pred_week: Prediction week\n    \"\"\"\n    pred_week = convert_to_epiweek(pred_week)\n    # Import smoothed dataframe\n    df = load_county_df(county, pred_week, pred_week + 4)\n    new_cases = df.loc[:, \"cases\"].values\n    new_deaths = df.loc[:, \"deaths\"].values\n    return new_cases, new_deaths\n\n########################################################\n#           FLU: regional/state/national level data\n########################################################\n\ndef load_df_flu(region, ew_start_data, ew_end_data, geo):\n    \"\"\"Load and clean data\"\"\"\n    if geo == \"hhs\":\n        datapath = datapath_flu_hhs\n    elif geo == \"state\":\n        datapath = datapath_flu_state\n    else:\n        raise ValueError(\"geo must be hhs or state\")\n    df = pd.read_csv(datapath, low_memory=False)\n\n    df = df[(df[\"region\"] == region)]\n    df[\"epiweek\"] = df.loc[:, \"epiweek\"].apply(convert_to_epiweek)\n    # Subset data using init parameters\n    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n    df = df.fillna(method=\"ffill\")\n    df = df.fillna(method=\"backfill\")\n    df = df.fillna(0)\n    return df\n\n\ndef get_state_train_data_flu(\n    region, pred_week, ew_start_data=EW_START_DATA_FLU, geo=\"state\", noise_level=0\n):\n    \"\"\"Get processed dataframe of data + target as array\"\"\"\n    # Import data\n    region = str.upper(region)\n    pred_week = convert_to_epiweek(pred_week)\n    ew_start_data = convert_to_epiweek(ew_start_data)\n    df = load_df_flu(region, ew_start_data, pred_week, geo)\n    # Select targets\n    targets = df[\"ili\"].astype(float).values.reshape(-1, 1)  # We need this 2d\n    if noise_level > 0:\n        # noise_level is an index for your list\n        noise = NOISE_LEVELS_FLU[noise_level - 1]\n        NOISE_STD = targets.std() * noise\n        noise_dist = np.random.normal(loc=0, scale=NOISE_STD, size=targets.shape)\n        noisy_targets = targets + noise_dist\n        targets = np.array([max(ix, 0) for ix in noisy_targets])\n    # Now subset based on input ew_start_data\n    df = df[[\"month\"] + include_cols]\n    return df, targets\n\n\ndef get_state_test_data_flu(region, pred_week, geo=\"state\"):\n    \"\"\"\n    @param pred_week: Prediction week\n    \"\"\"\n    pred_week = convert_to_epiweek(pred_week)\n    # Import smoothed dataframe\n    df = load_df_flu(region, pred_week + 1, pred_week + 4, geo)\n    ili = df.loc[:, \"ili\"].values\n    return ili\n\n\ndef get_dir_from_path_list(path):\n    outdir = path[0]\n    if not (os.path.exists(outdir)):\n        os.makedirs(outdir)\n    for p in path[1:]:\n        outdir = os.path.join(outdir, p)\n        if not (os.path.exists(outdir)):\n            os.makedirs(outdir)\n    return outdir","metadata":{"execution":{"iopub.status.busy":"2023-05-20T11:48:41.735895Z","iopub.execute_input":"2023-05-20T11:48:41.736253Z","iopub.status.idle":"2023-05-20T11:48:41.781323Z","shell.execute_reply.started":"2023-05-20T11:48:41.736227Z","shell.execute_reply":"2023-05-20T11:48:41.780227Z"},"trusted":true},"execution_count":159,"outputs":[]},{"cell_type":"markdown","source":"# Model Utils","metadata":{}},{"cell_type":"code","source":"SMOOTH_WINDOW = 7\n\nclass TransformerAttn(nn.Module):\n    \"\"\"\n    Module that calculates self-attention weights using transformer-like attention\n    \"\"\"\n\n    dim_in: int\n    value_dim: int\n    key_dim: int\n\n    def setup(self):\n        self.value_layer = nn.Dense(features=self.value_dim)\n        self.query_layer = nn.Dense(features=self.value_dim)\n        self.key_layer = nn.Dense(features=self.key_dim)\n\n    def __call__(self, seq):\n        seq_in = seq.transpose((1, 0, 2))\n        value = self.value_layer(seq_in)\n        query = self.query_layer(seq_in)\n        keys = self.key_layer(seq_in)\n        weights = (jnp.matmul(value, query.transpose((0, 2, 1)))) / jnp.sqrt(seq.shape[-1])\n        weights = nn.softmax(weights, axis=-1)\n        return jnp.matmul(weights, keys).transpose((1, 0, 2))\n\n    def forward_mask(self, seq, mask):\n        seq_in = seq.transpose((1, 0, 2))\n        value = self.value_layer(seq_in)\n        query = self.query_layer(seq_in)\n        keys = self.key_layer(seq_in)\n        weights = (jnp.matmul(value, query.transpose((0, 2, 1)))) / jnp.sqrt(seq.shape[-1])\n        weights = jnp.exp(weights)\n        weights = (weights.transpose((1, 2, 0)) * mask.transpose((1, 0))).transpose((1, 2, 0))\n        weights = weights / (weights.sum(-1, keepdims=True))\n        return jnp.matmul(weights, keys).transpose((1, 0, 2)) * mask\n\n\nclass EmbedAttenSeq(nn.Module):\n    \"\"\"\n    Module to embed a sequence. Adds Attention module.\n    \"\"\"\n\n    dim_seq_in: int\n    dim_metadata: int\n    rnn_out: int\n    dim_out: int\n    n_layers: int\n    bidirectional: bool\n    attn = TransformerAttn\n    dropout: float\n\n    def setup(self):\n        self.rnn = nn.GRU(\n            self.rnn_out // 2 if self.bidirectional else self.rnn_out,\n            num_layers=self.n_layers,\n            bidirectional=self.bidirectional,\n            dropout=self.dropout,\n        )\n        self.attn_layer = self.attn()\n        self.out_layer = nn.Sequential(\n            nn.Dense(features=self.dim_out),\n            nn.Tanh(),\n            nn.Dropout(rate=self.dropout),\n        )\n\n    def forward_mask(self, seqs, metadata, mask):\n        latent_seqs = self.rnn(seqs)[0]\n        latent_seqs = latent_seqs\n        latent_seqs = self.attn_layer.forward_mask(latent_seqs, mask)\n        latent_seqs = jnp.sum(latent_seqs, axis=0)\n        out = self.out_layer(jnp.concatenate([latent_seqs, metadata], axis=1))\n        return out\n\n    def forward(self, seqs, metadata):\n        latent_seqs, encoder_hidden = self.rnn(seqs)\n        latent_seqs = self.attn_layer(latent_seqs).sum(0)\n        out = self.out_layer(jnp.concatenate([latent_seqs, metadata], axis=1))\n        return out, encoder_hidden\n\nclass DecodeSeq(nn.Module):\n    \"\"\"\n    Module to embed a sequence. Adds Attention module.\n    \"\"\"\n\n    dim_seq_in: int\n    dim_metadata: int\n    rnn_out: int\n    dim_out: int\n    n_layers: int\n    bidirectional: bool\n    dropout: float\n\n    def setup(self):\n        self.act_fcn = nn.Tanh()\n        self.embed_input = nn.Dense(features=self.rnn_out)\n        self.attn_combine = nn.Dense(features=self.rnn_out)\n        self.rnn = nn.GRU(\n            self.rnn_out // 2 if self.bidirectional else self.rnn_out,\n            num_layers=self.n_layers,\n            bidirectional=self.bidirectional,\n            dropout=self.dropout,\n        )\n        self.out_layer = nn.Sequential(\n            nn.Dense(features=self.dim_out),\n            nn.Tanh(),\n            nn.Dropout(rate=self.dropout),\n        )\n\n    def forward(self, Hi_data, encoder_hidden, context):\n        inputs = Hi_data.transpose((1, 0, 2))\n        if self.bidirectional:\n            h0 = encoder_hidden[2:].transpose((1, 0, 2))\n        else:\n            h0 = jnp.stack(encoder_hidden[2:], axis=0).sum(0).unsqueeze(0)\n        inputs = self.embed_input(inputs)\n        context = jnp.tile(context, (inputs.shape[0], 1, 1))\n        inputs = jnp.concatenate((inputs, context), axis=2)\n        inputs = self.attn_combine(inputs)\n        latent_seqs, _ = self.rnn(inputs, h0)\n        latent_seqs = latent_seqs.transpose((1, 0, 2))\n        latent_seqs = self.out_layer(latent_seqs)\n        return latent_seqs\n\n\ndef moving_average(x, w):\n    return jnp.array(pd.Series(x).rolling(w, min_periods=1).mean().values)\n\n\ndef fetch_county_data_covid(\n    state=\"MA\", county_id=\"25005\", pred_week=\"202021\", batch_size=32, noise_level=0\n):\n    np.random.seed(17)\n\n    if county_id == \"all\":\n        all_counties = counties[state]\n    else:\n        all_counties = [county_id]\n\n    c_seqs = []\n    c_ys = []\n    for county in all_counties:\n        X_county, y = get_county_train_data(county, pred_week, noise_level=noise_level)\n        y = moving_average(y[:, 1].ravel(), SMOOTH_WINDOW).reshape(-1, 1)\n        c_seqs.append(X_county.to_numpy())\n        c_ys.append(y)\n    c_seqs = jnp.array(c_seqs)\n    c_ys = jnp.array(c_ys)\n\n    scalers = [StandardScaler() for _ in range(len(all_counties))]\n    c_seqs_norm = []\n    for i, scaler in enumerate(scalers):\n        c_seqs_norm.append(scaler.fit_transform(c_seqs[i]))\n    c_seqs_norm = jnp.array(c_seqs_norm)\n\n    county_idx = {r: i for i, r in enumerate(all_counties)}\n\n    def one_hot(idx, dim=len(county_idx)):\n        ans = jnp.zeros(dim, dtype=jnp.float32)\n        ans = ans.at[idx].set(1.0)\n        return ans\n\n    metadata = jnp.array([one_hot(county_idx[r]) for r in all_counties])\n\n    min_sequence_length = 5\n    metas, seqs, y, y_mask = [], [], [], []\n    for meta, seq, ys in zip(metadata, c_seqs_norm, c_ys):\n        seq, ys, ys_mask = create_window_seqs(seq, ys, min_sequence_length)\n        metas.append(meta)\n        seqs.append(seq[(-1,)])  # Modified indexing here\n        y.append(ys[(-1,)])  # Modified indexing here\n        y_mask.append(ys_mask[(-1,)])  # Modified indexing here\n\n    all_metas = jnp.array(metas, dtype=jnp.float32)\n    all_county_seqs = jnp.concatenate(seqs, axis=0)\n    all_county_ys = jnp.concatenate(y, axis=0)\n    all_county_y_mask = jnp.concatenate(y_mask, axis=0)\n\n    counties_train, metas_train, X_train, y_train, y_mask_train = (\n        all_counties,\n        all_metas,\n        all_county_seqs,\n        all_county_ys,\n        all_county_y_mask,\n    )\n\n    train_dataset = SeqData(counties_train, metas_train, X_train, y_train, y_mask_train)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n\n    seqlen = all_county_seqs.shape[1]\n    return train_loader, metas_train.shape[-1], X_train.shape[-1], seqlen\n\n\n\nclass SeqData(torch.utils.data.Dataset):\n    def __init__(self, counties, metas, seqs, ys, ys_mask):\n        self.counties = counties\n        self.metas = metas\n        self.seqs = seqs\n        self.ys = ys\n        self.ys_mask = ys_mask\n\n    def __len__(self):\n        return len(self.counties)\n\n    def __getitem__(self, index):\n        county = self.counties[index]\n        meta = self.metas[index]\n        seq = self.seqs[index]\n        y = self.ys[index]\n        y_mask = self.ys_mask[index]\n        return county, meta, seq, y, y_mask\n\nclass ODE(nn.Module):\n    def __init__(self, params, device):\n        super(ODE, self).__init__()\n        county_id = params[\"county_id\"]\n        abm_params = f\"/kaggle/input/gradabmdata/Data/{county_id}_generated_params.yaml\"\n        # Reading params\n        with open(abm_params, \"r\") as stream:\n            try:\n                abm_params = yaml.safe_load(stream)\n            except yaml.YAMLError as exc:\n                print(\"Error in reading parameters file\")\n                print(exc)\n        params.update(abm_params)\n        self.params = params\n        self.device = device\n        self.num_agents = self.params[\"num_agents\"]  # Population\n\n\nclass SEIRM(ODE):\n    def __init__(self, params, device):\n        super().__init__(params, device)\n\n    def init_compartments(self, learnable_params):\n        \"\"\"let's get initial conditions\"\"\"\n        initial_infections_percentage = learnable_params[\n            \"initial_infections_percentage\"\n        ]\n        initial_conditions = jnp.empty((5))\n        no_infected = (\n            initial_infections_percentage / 100\n        ) * self.num_agents  # 1.0 is ILI\n        initial_conditions = initial_conditions.at[2].set(no_infected)\n        initial_conditions = initial_conditions.at[0].set( self.num_agents - no_infected)\n        print(\"initial infected\", no_infected)\n        self.state = initial_conditions\n\n    def step(self, t, values):\n        \"\"\"\n        Computes ODE states via equations\n            state is the array of state value (S,E,I,R,M)\n        \"\"\"\n        params = {\n            \"beta\": values[0],\n            \"alpha\": values[1],\n            \"gamma\": values[2],\n            \"mu\": values[3],\n            \"initial_infections_percentage\": values[4],\n        }\n        if t == 0:\n            self.init_compartments(params)\n        # to make the NN predict lower numbers, we can make its prediction to be N-Susceptible\n        dSE = params[\"beta\"] * self.state[0] * self.state[2] / self.num_agents\n        dEI = params[\"alpha\"] * self.state[1]\n        dIR = params[\"gamma\"] * self.state[2]\n        dIM = params[\"mu\"] * self.state[2]\n\n        dS = -1.0 * dSE\n        dE = dSE - dEI\n        dI = dEI - dIR - dIM\n        dR = dIR\n        dM = dIM\n\n        # concat and reshape to make it rows as obs, cols as states\n        self.dstate = jnp.stack([dS, dE, dI, dR, dM], 0)\n        NEW_INFECTIONS_TODAY = dEI\n        NEW_DEATHS_TODAY = dIM\n        # update state\n        self.state = self.state + self.dstate\n\n        return NEW_INFECTIONS_TODAY, NEW_DEATHS_TODAY\n\nclass SIRS(ODE):\n    def __init__(self, params, device):\n        super().__init__(params, device)\n\n    def init_compartments(self, learnable_params):\n        \"\"\"let's get initial conditions\"\"\"\n        initial_infections_percentage = learnable_params[\n            \"initial_infections_percentage\"\n        ]\n        initial_conditions = jnp.empty((2))\n        no_infected = (\n            initial_infections_percentage / 100\n        ) * self.num_agents  # 1.0 is ILI\n        initial_conditions = initial_conditions.at[1].set(no_infected)\n        initial_conditions = initial_conditions.at[0].set(self.num_agents - no_infected)\n        print(\"initial infected\", no_infected)\n\n        self.state = initial_conditions\n\n    def step(self, t, values):\n        \"\"\"\n        Computes ODE states via equations\n            state is the array of state value (S,I)\n        \"\"\"\n        params = {\n            \"beta\": values[0],  # contact rate, range: 0-1\n            \"initial_infections_percentage\": values[1],\n        }\n        # set from expertise\n        params[\"D\"] = 3.5\n        params[\"L\"] = 2000\n        if t == 0:\n            self.init_compartments(params)\n        dS = (self.num_agents - self.state[0] - self.state[1]) / params[\"L\"] - params[\n            \"beta\"\n        ] * self.state[0] * self.state[1] / self.num_agents\n        dSI = params[\"beta\"] * self.state[0] * self.state[1] / self.num_agents\n        dI = dSI - self.state[1] / params[\"D\"]\n\n        # concat and reshape to make it rows as obs, cols as states\n        self.dstate = jnp.stack([dS, dI], 0)\n\n        NEW_INFECTIONS_TODAY = dSI\n        # ILI is percentage of outpatients with influenza-like illness\n        # ILI = params['lambda'] * dSI / self.num_agents\n        ILI = dSI / self.num_agents * 100  # multiply 100 because it is percentage\n\n        # update state\n        self.state = self.state + self.dstate\n        return NEW_INFECTIONS_TODAY, ILI\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T11:48:42.075244Z","iopub.execute_input":"2023-05-20T11:48:42.075617Z","iopub.status.idle":"2023-05-20T11:48:42.135901Z","shell.execute_reply.started":"2023-05-20T11:48:42.075586Z","shell.execute_reply":"2023-05-20T11:48:42.134822Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"markdown","source":"# Abm-model","metadata":{}},{"cell_type":"code","source":"from jax import random, lax\nimport flax.linen as nn\n\nclass LogitRelaxedBernoulli(nn.Module):\n    temperature: float = 0.3\n\n    def setup(self):\n        self.temperature = self.param(\"temperature\", nn.initializers.constant(self.temperature))\n\n    def rsample(self, rng_key, shape):\n        eps = random.uniform(rng_key, shape, minval=1e-6, maxval=1 - 1e-6)\n        logits = self.logits\n        y = (logits + jnp.log(eps) - jnp.log(1.0 - eps)) / self.temperature\n        return y\n\n    def log_prob(self, value):\n        return (\n            jnp.log(self.temperature)\n            - self.temperature * value\n            + self.logits\n            - 2 * nn.softplus(-self.temperature * value + self.logits)\n        )\n\nclass InfectionNetwork(MessagePassing):\n    lam: nn.Module\n    R: nn.Module\n    SFSusceptibility: jnp.ndarray\n    SFInfector: jnp.ndarray\n    lam_gamma_integrals: jnp.ndarray\n\n    def setup(self, lam,R,SFSusceptibility,SFInfector,lam_gamma_integrals):\n        self.lam = lam\n        self.R = R\n        self.SFSusceptibility = SFSusceptibility\n        self.SFInfector = SFInfector\n        self.lam_gamma_integrals = lam_gamma_integrals\n\n    def forward_sparse(self, data, r0_value_trainable):\n        x = data.x\n        edge_index = data.edge_index\n        edge_attr = data.edge_attr\n        t = data.t\n        # sparse adjacency matrix of inter-agent interactions\n        S_A_s = self.SFSusceptibility[x[:, 0].astype(jnp.int32)]\n        A_s_i = self.SFInfector[x[:, 1].astype(jnp.int32)]\n        integrals = jnp.zeros_like(S_A_s)\n        infected_idx = x[:, 2].astype(jnp.bool_)\n        infected_times = t - x[infected_idx, 3]\n        integrals = integrals.at[infected_idx].set(self.lam_gamma_integrals[infected_times.astype(jnp.int32)],)\n        I_bar = x[:, 4 + 22]  # only info for random network being used in current expts\n        integral_asi = A_s_i * integrals\n        sparse_adj = jax.scipy.sparse.coo_matrix(\n            (jnp.ones(edge_index.shape[1]), edge_index),\n            shape=(x.shape[0], x.shape[0])\n        ).tocsr()\n        sparse_asi = jax.scipy.sparse.coo_matrix(\n            (integral_asi.reshape(-1), (edge_index[0], edge_index[1])),\n            shape=(x.shape[0], 1)\n        ).tocsr()\n        sparse_mult = sparse_adj @ sparse_asi\n        dense_mult = sparse_mult.toarray().reshape(-1)\n\n        # total infection\n        infection_transmission = (\n            r0_value_trainable * S_A_s * dense_mult\n        ) / I_bar  # /I_bar\n        return infection_transmission.reshape(1, -1)\n\n    def forward(self, data, r0_value_trainable):\n        x = data.x\n        edge_index = data.edge_index\n        edge_attr = data.edge_attr\n        t = data.t\n        return self.propagate(\n            edge_index,\n            x=x,\n            edge_attr=edge_attr,\n            t=t,\n            R=r0_value_trainable,\n            SFSusceptibility=self.SFSusceptibility,\n            SFInfector=self.SFInfector,\n            lam_gamma_integrals=self.lam_gamma_integrals,\n        )\n\n    def message(\n        self,\n        x_i,\n        x_j,\n        edge_attr,\n        t,\n        R,\n        SFSusceptibility,\n        SFInfector,\n        lam_gamma_integrals,\n    ):\n        # x_j has shape [E, in_channels]\n        tmp = self.lam(\n            x_i, x_j, edge_attr, t, R, SFSusceptibility, SFInfector, lam_gamma_integrals\n        )  # tmp has shape [E, 2 * in_channels]\n        return tmp\n\nfrom abc import ABC, abstractmethod\n\nclass DiseaseProgression(ABC, nn.Module):\n    \"\"\"Abstract class for disease progression\"\"\"\n\n    @abstractmethod\n    def initialize_variables(self, agents_infected_time, agents_stages, agents_next_stage_times):\n        \"\"\"Initialize tensor variables depending on the disease\"\"\"\n        pass\n\n    @abstractmethod\n    def update_next_stage_times(self, learnable_params, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        \"\"\"Update the time for the next stage\"\"\"\n        pass\n\n    @abstractmethod\n    def update_current_stage(self, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        \"\"\"Update the current disease stage\"\"\"\n        pass\n\nclass SEIRMProgression(DiseaseProgression):\n    \"\"\"SEIRM for COVID-19\"\"\"\n\n    SUSCEPTIBLE_VAR = 0\n    EXPOSED_VAR = 1  # exposed state\n    INFECTED_VAR = 2\n    RECOVERED_VAR = 3\n    MORTALITY_VAR = 4\n\n    def __init__(self, params):\n        super().__init__()\n        # Default times (only for initialization, later they are learned)\n        self.EXPOSED_TO_INFECTED_TIME = 3\n        self.INFECTED_TO_RECOVERED_TIME = 5\n        self.INFINITY_TIME = params[\"num_steps\"] + 1\n        self.num_agents = params[\"num_agents\"]\n\n    def initialize_variables(self, agents_infected_time, agents_stages, agents_next_stage_times):\n        \"\"\"Initialize tensor variables depending on the disease\"\"\"\n        agents_infected_time = agents_infected_time.at[jax.numpy.logical_or(agents_stages == self.EXPOSED_VAR, agents_stages == self.INFECTED_VAR)].set(-1)\n        agents_next_stage_times = agents_next_stage_times.at[agents_stages == self.EXPOSED_VAR].set(self.EXPOSED_TO_INFECTED_TIME)\n        agents_next_stage_times = agents_next_stage_times.at[agents_stages == self.INFECTED_VAR].set(-1 * self.EXPOSED_TO_INFECTED_TIME)\n        return agents_infected_time, agents_next_stage_times\n\n    def update_next_stage_times(self, learnable_params, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        \"\"\"Update the time for the next stage\"\"\"\n        exposed_to_infected_time = learnable_params[\"exposed_to_infected_time\"]\n        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n        new_transition_times = agents_next_stage_times.copy()\n        new_transition_times = new_transition_times.at[jnp.logical_and(current_stages == self.INFECTED_VAR, agents_next_stage_times == t)].set(self.INFINITY_TIME)\n        new_transition_times = new_transition_times.at[jnp.logical_and(current_stages == self.EXPOSED_VAR, agents_next_stage_times == t)].set(t + infected_to_recovered_time)\n        return jnp.where(\n            newly_exposed_today,\n            (t + 1 + exposed_to_infected_time),\n            new_transition_times\n        )\n\n    def update_current_stage(self, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        \"\"\"Update the current disease stage\"\"\"\n        transition_to_infected = jnp.where(agents_next_stage_times <= t, self.INFECTED_VAR, self.EXPOSED_VAR)\n        transition_to_mortality_or_recovered = jnp.where(agents_next_stage_times <= t, self.RECOVERED_VAR, self.INFECTED_VAR)\n        stage_progression = jnp.where(\n            current_stages == self.SUSCEPTIBLE_VAR,\n            self.SUSCEPTIBLE_VAR,\n            jnp.where(\n                current_stages == self.RECOVERED_VAR,\n                self.RECOVERED_VAR,\n                jnp.where(\n                    current_stages == self.MORTALITY_VAR,\n                    self.MORTALITY_VAR,\n                    jnp.where(\n                        current_stages == self.EXPOSED_VAR,\n                        transition_to_infected,\n                        transition_to_mortality_or_recovered\n                    )\n                )\n            )\n        )\n        current_stages = jnp.where(\n            newly_exposed_today,\n            self.EXPOSED_VAR,\n            stage_progression\n        )\n        return current_stages\n\n    def init_stages(self, learnable_params, device):\n        \"\"\"Initialize the stages\"\"\"\n        initial_infections_percentage = learnable_params[\"initial_infections_percentage\"]\n        prob_infected = (initial_infections_percentage / 100) * jnp.ones((self.num_agents, 1), dtype=jnp.float32)\n        p = jnp.concatenate((prob_infected, 1 - prob_infected), axis=1)\n        cat_logits = jnp.log(p + 1e-9)\n        agents_stages = jax.nn.gumbel_softmax(cat_logits, tau=1, hard=True, axis=1)[:, 0]\n        return agents_stages\n\nclass SIRSProgression(DiseaseProgression):\n    \"\"\"SIRS for influenza\"\"\"\n\n    SUSCEPTIBLE_VAR = 0\n    INFECTED_VAR = 1\n    RECOVERED_VAR = 2\n\n    def __init__(self, params):\n        super().__init__()\n        # Default times (only for initialization, later they are learned)\n        self.INFECTED_TO_RECOVERED_TIME = 5\n        self.RECOVERED_TO_SUSCEPTIBLE_TIME = 100\n        self.INFINITY_TIME = params[\"num_steps\"] + 1\n        self.num_agents = params[\"num_agents\"]\n\n    def initialize_variables(self, agents_infected_time, agents_stages, agents_next_stage_times):\n        \"\"\"Initialize tensor variables depending on the disease\"\"\"\n        agents_infected_time = agents_infected_time.at[agents_stages == self.INFECTED_VAR].set(-1)\n        agents_next_stage_times = agents_next_stage_times.at[agents_stages == self.INFECTED_VAR].set(self.INFECTED_TO_RECOVERED_TIME)\n        return agents_infected_time, agents_next_stage_times\n\n    def update_initial_times(self, learnable_params, agents_stages, agents_next_stage_times):\n        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n        new_transition_times = agents_next_stage_times.set[jnp.logical_and(agents_stages == self.INFECTED_VAR, agents_next_stage_times <= t)].set(\n            infected_to_recovered_time\n        )\n        return new_transition_times\n\n    def get_newly_exposed(self, current_stages, potentially_exposed_today):\n        newly_exposed_today = jnp.logical_and(current_stages == self.SUSCEPTIBLE_VAR, potentially_exposed_today)\n        return newly_exposed_today\n\n    def update_next_stage_times(self, learnable_params, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n        recovered_to_susceptible_time = learnable_params[\"recovered_to_susceptible_time\"]\n        new_transition_times = jnp.where(\n            jnp.logical_and(current_stages == self.INFECTED_VAR, agents_next_stage_times == t),\n            (t + recovered_to_susceptible_time),\n            agents_next_stage_times\n        )\n        return jnp.where(\n            newly_exposed_today,\n            (t + 1 + infected_to_recovered_time),\n            new_transition_times\n        )\n\n    def get_target_variables(self, params, learnable_params, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        new_recovered_today = jnp.where(\n            jnp.logical_and(current_stages == self.INFECTED_VAR, agents_next_stage_times <= t),\n            self.RECOVERED_VAR,\n            self.INFECTED_VAR\n        )\n        ILI = jnp.sum(newly_exposed_today) / params[\"num_agents\"] * 100\n        NEW_INFECTIONS_TODAY = jnp.sum(newly_exposed_today)\n        return new_recovered_today, NEW_INFECTIONS_TODAY, ILI\n\n    def update_current_stage(self, newly_exposed_today, current_stages, agents_next_stage_times, t):\n        transition_to_recovered = jnp.where(agents_next_stage_times <= t, self.RECOVERED_VAR, self.INFECTED_VAR)\n        transition_to_susceptible = jnp.where(agents_next_stage_times <= t, self.SUSCEPTIBLE_VAR, self.RECOVERED_VAR)\n        stage_progression = jnp.where(\n            newly_exposed_today,\n            self.INFECTED_VAR,\n            jnp.where(\n                current_stages == self.SUSCEPTIBLE_VAR,\n                self.SUSCEPTIBLE_VAR,\n                jnp.where(\n                    current_stages == self.RECOVERED_VAR,\n                    self.RECOVERED_VAR,\n                    jnp.where(\n                        current_stages == self.INFECTED_VAR,\n                        transition_to_recovered,\n                        transition_to_susceptible\n                    )\n                )\n            )\n        )\n        current_stages = jnp.where(\n            newly_exposed_today,\n            self.INFECTED_VAR,\n            stage_progression\n        )\n        return current_stages\n\n    def init_stages(self, learnable_params, device):\n        initial_infections_percentage = learnable_params[\"initial_infections_percentage\"]\n        prob_infected = (initial_infections_percentage / 100) * jnp.ones((self.num_agents, 1), dtype=jnp.float32)\n        p = jnp.concatenate((prob_infected, 1 - prob_infected), axis=1)\n        cat_logits = jnp.log(p + 1e-9)\n        agents_stages = jax.nn.gumbel_softmax(cat_logits, tau=1, hard=True, axis=1)[:, 0]\n        return agents_stages\n\nimport os\nimport yaml\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax import linen as nn\nfrom scipy.stats import gamma\nfrom functools import partial\n\nclass GradABM:\n    def __init__(self, params, device):\n        county_id = params[\"county_id\"]\n        abm_params = f\"/kaggle/input/gradabmdata/Data/ABM_parameters/{county_id}_generated_params.yaml\"\n\n        # Reading params\n        with open(abm_params, \"r\") as stream:\n            try:\n                abm_params = yaml.safe_load(stream)\n            except yaml.YAMLError as exc:\n                print(\"Error in reading parameters file\")\n                print(exc)\n    \n        params.update(abm_params)\n        params[\"output_location\"][\"parent_dir\"] = \"/kaggle/input/gradabmdata/Data\"\n        self.params = params\n        self.device = device\n        self.num_agents = self.params[\"num_agents\"]\n        print(\"Num Agents: \", self.num_agents)\n\n        # **********************************************************************************\n        # Environment state variables\n        # **********************************************************************************\n        # **********************************************************************************\n        # Static\n        self.agents_ix = jnp.arange(0, self.params[\"num_agents\"])\n        infile = os.path.join(\n            self.params[\"output_location\"][\"parent_dir\"],\n            self.params[\"output_location\"][\"agents_dir\"],\n            self.params[\"output_location\"][\"agents_outfile\"],\n        )\n\n        agents_df = pd.read_csv(infile)\n        self.agents_ages = jnp.array(agents_df[\"age_group\"].to_numpy())\n\n        self.num_networks = 23\n        self.network_type_dict = {}\n        self.network_type_dict[\"random\"] = 22\n        self.network_type_dict_inv = {}\n        self.network_type_dict_inv[22] = \"random\"\n        \n        params['disease'] = \"COVID\"\n        # select disease progression model\n        if params['disease'] == \"COVID\":\n            self.DPM = SEIRMProgression(params)\n        elif params['disease'] == \"Flu\":\n            self.DPM = SIRSProgression(params)\n\n        # Age and Network and Occupation may need to be checked to populate this\n        self.agents_mean_interactions = jnp.zeros(\n            (self.params[\"num_agents\"], self.num_networks)\n        )\n\n        mean_int_ran_df = pd.read_csv(\"/kaggle/input/gradabmdata/Data/Initialization/RandomNetworkParameters.csv\")\n        mean_int_ran_mu = jnp.array(mean_int_ran_df[\"mu\"].values)\n\n        child_agents = self.agents_ages <= self.params[\"CHILD_Upper_Index\"]\n        adult_agents = jnp.logical_and(\n            self.agents_ages > self.params[\"CHILD_Upper_Index\"],\n            self.agents_ages <= self.params[\"ADULT_Upper_Index\"],\n        ).reshape(-1)\n        elderly_agents = self.agents_ages > self.params[\"ADULT_Upper_Index\"]\n        self.agents_mean_interactions = self.agents_mean_interactions.at[jax.ops.index[child_agents, 22]].set(mean_int_ran_mu[0])\n        self.agents_mean_interactions = self.agents_mean_interactions.at[jax.ops.index[adult_agents, 22]].set(mean_int_ran_mu[1]\n        )\n        self.agents_mean_interactions = self.agents_mean_interactions.at[jax.ops.index[elderly_agents, 22]].set( mean_int_ran_mu[2])\n        self.agents_mean_interactions_split = [\n            a.reshape(-1) for a in jnp.split(self.agents_mean_interactions, 1, axis=1)\n        ]\n\n        self.R = 5.18  # learnable, but the default value\n        self.R = jnp.array(self.R)\n        if self.params['disease'] == \"COVID\":\n            self.SFSusceptibility = jnp.array([0.35, 0.69, 1.03, 1.03, 1.03, 1.03, 1.27, 1.52, 1.52], dtype=jnp.float32)\n            self.SFInfector = jnp.array([0.0, 0.33, 0.72, 0.0, 0.0], dtype=jnp.float32)\n            self.lam_gamma = {}\n            self.lam_gamma[\"scale\"] = 5.5\n            self.lam_gamma[\"rate\"] = 2.14\n        elif self.params['disease'] == \"Flu\":\n            # from CDC Table 1, median value: https://www.cdc.gov/flu/about/keyfacts.htm\n            self.SFSusceptibility = jnp.array([13.2, 7.9, 7.4, 7.4, 7.4, 12.0, 12.0, 3.9, 3.9], dtype=jnp.float32)\n            self.SFSusceptibility = nn.softmax(self.SFSusceptibility, axis=0) * 25\n            self.SFInfector = jnp.array([0.0, 0.72, 0.0], dtype=jnp.float32)\n            self.lam_gamma = {}\n            self.lam_gamma[\"scale\"] = 7.5  # mean is scale/rate, CDC says it's 3-4 days\n            self.lam_gamma[\"rate\"] = 2.14\n\n        self.B_n = {}\n        self.B_n[\"household\"] = 2\n        self.B_n[\"occupation\"] = 1\n        self.B_n[\"random\"] = 1  # 0.25\n\n        self.lam_gamma_integrals = self._get_lam_gamma_integrals(\n            **self.lam_gamma, t=self.params[\"num_steps\"] + 10\n        )  # add 10 to make sure we cover all\n        self.lam_gamma_integrals = self.lam_gamma_integrals\n\n        self.net = InfectionNetwork(\n            lam,\n            self.R,\n            self.SFSusceptibility,\n            self.SFInfector,\n            self.lam_gamma_integrals,\n            self.device,\n        )\n\n        self.current_time = 0\n        # **********************************************************************************\n        self.all_edgelist, self.all_edgeattr = self.init_interaction_graph(\n            t=0\n        )  # get one initial interaction graph\n\n    def init_interaction_graph(self, t):\n        \"\"\"this is Part-1 of Step\"\"\"\n        \n        infile = os.path.join(\n            get_dir_from_path_list(\n                [\n                    self.params[\"output_location\"][\"parent_dir\"],\n                    self.params[\"output_location\"][\"networks_dir\"],\n                    self.params[\"output_location\"][\"random_networks_dir\"],\n                ]\n            ),\n            \"{}.csv\".format(t),\n        )\n\n        random_network_edgelist_forward = jnp.array(pd.read_csv(infile, header=None).to_numpy().T, dtype=jnp.int32)\n        random_network_edgelist_backward = jnp.vstack(\n            (\n                random_network_edgelist_forward[1, :],\n                random_network_edgelist_forward[0, :],\n            )\n        )\n        random_network_edgelist = jnp.hstack(\n            (random_network_edgelist_forward, random_network_edgelist_backward)\n        )\n        random_network_edgeattr_type = jnp.ones(random_network_edgelist.shape[1], dtype=jnp.int32) * self.network_type_dict[\"random\"]\n\n        random_network_edgeattr_B_n = jnp.ones(random_network_edgelist.shape[1], dtype=jnp.float32) * self.B_n[\"random\"]\n        random_network_edgeattr = jnp.vstack(\n            (random_network_edgeattr_type, random_network_edgeattr_B_n)\n        )\n\n        all_edgelist = jnp.hstack((random_network_edgelist,))\n        all_edgeattr = jnp.hstack((random_network_edgeattr,))\n\n        return all_edgelist, all_edgeattr\n\n    def get_interaction_graph(self, t):\n        return self.all_edgelist, self.all_edgeattr\n\n    def init_state_tensors(self, learnable_params):\n        \"\"\"Initializing message passing network (currently no trainable parameters here)\"\"\"\n        # Dynamic\n        # a.Testing\n        # b.Quarantine\n        # c.Infection and Disease\n        self.current_stages = self.DPM.init_stages(learnable_params, self.device)\n        self.agents_infected_index = (self.current_stages > 0).astype(jnp.float32)  # Not susceptible\n        self.agents_infected_time = (\n            (self.params[\"num_steps\"] + 1) * jnp.ones_like(self.current_stages)\n        ).astype(jnp.float32)  # Practically infinite as np.inf gives wrong data type\n\n        self.agents_next_stages = -1 * jnp.ones_like(self.current_stages)\n        self.agents_next_stage_times = (self.params[\"num_steps\"] + 1) * jnp.ones_like(\n            self.current_stages\n        ).astype(jnp.int32)  # Practically infinite as np.inf gives wrong data type\n\n        # update values depending on the disease\n        (\n            self.agents_infected_time,\n            self.agents_next_stage_times,\n        ) = self.DPM.initialize_variables(\n            self.agents_infected_time, self.current_stages, self.agents_next_stage_times\n        )\n\n        self.agents_next_stage_times = self.agents_next_stage_times.astype(jnp.float32)\n        self.agents_infected_time = self.agents_infected_time.astype(jnp.float32)\n\n    def step(self, t, param_t):\n        \"\"\"Send as input: r0_value [hidden state] -> trainable parameters  and t is the time-step of simulation.\"\"\"\n        # construct dictionary with trainable parameters\n        learnable_params = {}\n        if self.params['disease'] == \"COVID\":\n            learnable_params[\"r0_value\"] = param_t[0]\n            learnable_params[\"mortality_rate\"] = param_t[1]\n            learnable_params[\"initial_infections_percentage\"] = param_t[2]\n            learnable_params[\"exposed_to_infected_time\"] = 3\n            learnable_params[\"infected_to_recovered_time\"] = 5\n        elif self.params['disease'] == \"Flu\":\n            learnable_params[\"r0_value\"] = param_t[0]\n            learnable_params[\"initial_infections_percentage\"] = param_t[1]\n            learnable_params[\"infected_to_recovered_time\"] = 3.5\n            learnable_params[\"recovered_to_susceptible_time\"] = 2000\n        \"\"\" change params that were set in constructor \"\"\"\n        if t == 0:\n            self.init_state_tensors(learnable_params)\n            self.agents_next_stage_times = self.DPM.update_initial_times(\n                learnable_params, self.current_stages, self.agents_next_stage_times\n            )\n\n        # t = self.current_time\n        \"\"\"Steps: i) Get interaction graph, ii) Message Passing of Infection iii) State Evolution \"\"\"\n\n        # ******************************************************************************** #\n        # Part-1. Interaction Graph - Output: EdgeList, EdgeFeatures and NodeFeatures\n        all_edgelist, all_edgeattr = self.get_interaction_graph(\n            t\n        )  # the interaction graphs for GNN at time t\n        all_nodeattr = jnp.stack(\n            (\n                self.agents_ages,  # 0\n                self.current_stages.detach(),  # 1\n                self.agents_infected_index.to(self.device),  # 2\n                self.agents_infected_time.to(self.device),  # 3\n                *self.agents_mean_interactions_split,  # 4 to 26\n                jnp.arange(self.params[\"num_agents\"]).to(\n                    self.device\n                ),  # Agent ids (27)\n            )\n        ).T\n        agents_data = Data(\n            all_nodeattr,\n            edge_index=all_edgelist,\n            edge_attr=all_edgeattr,\n            t=t,\n            agents_mean_interactions=self.agents_mean_interactions,\n        )\n\n        # ******************************************************************************** #\n        # Part-2. Message Passing - Transmission Dynamics + New Infections: {GNN + Variational Inference}\n        # agent steps: i) collects infection [GNN]; ii) get infected based on total infection collected [Variational Inference]\n        # message passing: collecting infection from neighbors\n        lam_t = self.net(agents_data, learnable_params[\"r0_value\"])\n        prob_not_infected = jnp.exp(-lam_t)\n        p = jnp.hstack((1 - prob_not_infected, prob_not_infected))\n        cat_logits = jnp.log(p + 1e-9)\n        potentially_exposed_today = nn.gumbel_softmax(\n            logits=cat_logits, tau=1, hard=True, axis=1\n        )[\n            :, 0\n        ]  # first column is prob of infections\n        newly_exposed_today = self.DPM.get_newly_exposed(\n            self.current_stages, potentially_exposed_today\n        )\n\n        # ******************************************************************************** #\n        # Part-3. State Evolution -> Progression Dynamics {Deterministic}\n        # to do here: i) update curr_stage; ii) update next_transition_time\n        # self.agents_infected_time[t].sum()\n        # check 2 things: i) got infected_today -> go from S to E; ii) already infected -> update E to I; I to R or M.\n        # stage_progression, new_death_recovered_today = self.deterministic_stage_transition(self.agents_stages[t,:],\n\n        # before updating, get target variables like new deaths or ILI\n        # also get recovered ones\n        recovered_dead_now, target1, target2 = self.DPM.get_target_variables(\n            self.params,\n            learnable_params,\n            newly_exposed_today,\n            self.current_stages,\n            self.agents_next_stage_times,\n            t,\n        )\n\n        # get next stages without updating yet the current_stages\n        next_stages = self.DPM.update_current_stage(\n            newly_exposed_today, self.current_stages, self.agents_next_stage_times, t\n        )\n\n        # update times with current_stages\n        self.agents_next_stage_times = self.DPM.update_next_stage_times(\n            learnable_params,\n            newly_exposed_today,\n            self.current_stages,\n            self.agents_next_stage_times,\n            t,\n        )\n\n        # safely update current_stages\n        self.current_stages = next_stages\n\n        # update for newly exposed agents {exposed_today}\n        self.agents_infected_index = self.agents_infected_index.at[newly_exposed_today.astype(jnp.bool_)].set(1)\n        self.agents_infected_time = self.agents_infected_time.at[newly_exposed_today.astype(jnp.bool_)].set(t)\n        self.current_time = t + 1\n\n        # ******************************************************************************** #\n        # TODO: Part-4. ILI/ILI-probability update for next step -> on newly_exposed_today and current_stages\n        # self.AILI_model.updateILI(current_stages.detach(), self.agents_mean_interactions, t)\n\n        # ******************************************************************************** #\n        # Wrap it all together\n        wrapped_output = [recovered_dead_now, target1, target2]\n        wrapped_output = [x for x in wrapped_output if x is not None]\n        wrapped_output = (\n            tuple(wrapped_output) if len(wrapped_output) > 1 else wrapped_output[0]\n        )\n\n        return wrapped_output\n\n    def forward(self, x):\n        # foward will be used for training and testing purposes\n        output = []\n        for i in range(self.params[\"num_steps\"]):\n            # inference time\n            if len(x) > 0:\n                output.append(self.step(i, x[i]))\n            else:\n                output.append(self.step(i, []))\n\n        return output\n\n    def process_output(self, output):\n        # process the output to return meaningful data\n        return output\n","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:20:29.059846Z","iopub.execute_input":"2023-05-20T12:20:29.060222Z","iopub.status.idle":"2023-05-20T12:20:29.154430Z","shell.execute_reply.started":"2023-05-20T12:20:29.060191Z","shell.execute_reply":"2023-05-20T12:20:29.153382Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"markdown","source":"# Train_abm","metadata":{}},{"cell_type":"code","source":"BENCHMARK_TRAIN = False\nNUM_EPOCHS_DIFF = 15\nprint(\"---- MAIN IMPORTS SUCCESSFUL -----\")\nepsilon = 1e-6\n\nMIN_VAL_PARAMS = {\n\"abm-covid\": [\n1.0,\n0.001,\n0.01,\n], # r0, mortality rate, initial_infections_percentage\n\"abm-flu\": [1.05, 0.1], # r0, initial_infections_percentage\n\"seirm\": [\n0.0,\n0.0,\n0.0,\n0.0,\n0.01,\n], # beta, alpha, gamma, mu, initial_infections_percentage\n\"sirs\": [0.0, 0.1], # beta, initial_infections_percentage\n}\nMAX_VAL_PARAMS = {\n\"abm-covid\": [8.0, 0.02, 1.0],\n\"abm-flu\": [2.6, 5.0],\n\"seirm\": [1.0, 1.0, 1.0, 1.0, 1.0],\n\"sirs\": [1.0, 5.0],\n}\n\nDAYS_HEAD = 4 * 7 # 4 weeks ahead\n\npi = jnp.array([math.pi], dtype=jnp.float32)\n\nSAVE_MODEL_PATH = \"/kaggle/working/Models/\"\n\n# neural network predicting parameters of the ABM\nimport jax.numpy as jnp\nimport flax.linen as nn\n\nclass CalibNN(nn.Module):\n    metas_train_dim: int\n    X_train_dim: int\n    device: str\n    training_weeks: int\n    hidden_dim: int = 32\n    out_dim: int = 1\n    n_layers: int = 2\n    scale_output: str = \"abm-covid\"\n    bidirectional: bool = True\n\n    def setup(self):\n        \"\"\" tune \"\"\"\n        hidden_dim = 64\n        out_layer_dim = 32\n\n        self.emb_model = EmbedAttenSeq(\n            dim_seq_in=self.X_train_dim,\n            dim_metadata=self.metas_train_dim,\n            rnn_out=self.hidden_dim,\n            dim_out=self.hidden_dim,\n            n_layers=self.n_layers,\n            dropout=0.1,\n            bidirectional=self.bidirectional,\n        )\n\n        self.decoder = DecodeSeq(\n            dim_seq_in=1,\n            rnn_out=self.hidden_dim,  # divides by 2 if bidirectional\n            dim_out=out_layer_dim,\n            n_layers=1,\n            dropout=0.1,\n            dim_metadata=1,\n            bidirectional=True,\n        )\n\n        out_layer_width = out_layer_dim\n        self.out_layer = [\n            nn.Dense(out_layer_width // 2),\n            nn.relu,\n            nn.Dense(self.out_dim),\n        ]\n\n        self.min_values = jnp.array(MIN_VAL_PARAMS[self.scale_output])\n        self.max_values = jnp.array(MAX_VAL_PARAMS[self.scale_output])\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x, meta):\n        x_embeds, encoder_hidden = self.emb_model.call(x.transpose((1, 0)), meta)\n        time_seq = jnp.arange(1, self.training_weeks + WEEKS_AHEAD + 1).repeat(x_embeds.shape[0], 1).unsqueeze(2)\n        Hi_data = ((time_seq - time_seq.min()) / (time_seq.max() - time_seq.min())).to(self.device)\n        emb = self.decoder.call(Hi_data, encoder_hidden, x_embeds)\n        out = self.out_layer(emb)\n        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n        return out\n\n    \nclass ParamModel(nn.Module):\n    def setup(self):\n        self.emb_model = EmbedAttenSeq(\n            dim_seq_in=self.X_train_dim,\n            dim_metadata=self.metas_train_dim,\n            dim_out=self.hidden_dim,\n            n_layers=self.n_layers,\n            dropout=0.1,\n            rnn_out=self.hidden_dim,\n            bidirectional=self.bidirectional,\n        )\n        self.layer1 = nn.Dense(features=20)\n        self.layer_bypass = nn.Dense(features=20)\n        self.meanfc = nn.Dense(features=self.out_dim, use_bias=True)\n        \n        if self.CUSTOM_INIT:\n            self.meanfc.bias = nn.initializers.ones  # Set the desired initialization value\n        \n    def __init__(\n        self,\n        metas_train_dim,\n        X_train_dim,\n        device,\n        hidden_dim=50,\n        n_layers=2,\n        out_dim=1,\n        scale_output=\"abm-covid\",\n        bidirectional=True,\n        CUSTOM_INIT=True,\n    ):\n        super().__init__()\n        self.device = device\n        self.metas_train_dim = metas_train_dim\n        self.X_train_dim = X_train_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.out_dim = out_dim\n        self.scale_output = scale_output\n        self.bidirectional = bidirectional\n        self.CUSTOM_INIT = CUSTOM_INIT\n        \n        self.init_weights()\n        \n    def init_weights(self):\n        for module in self.submodules():\n            if isinstance(module, nn.Dense):\n                nn.initializers.xavier_uniform()(module.weight)\n                module.bias = module.bias + 0.01  # Modify bias initialization if required\n        \n    def forward(self, x, meta):\n        x_embeds = self.emb_model(x.transpose(1, 0), meta)\n        \n        # use embedding for predicting: i) R0 and ii) Cases {for support counties} [FOR LATER]\n        ro_feats = self.layer1(x_embeds)\n        ro_feats = nn.ReLU()(ro_feats)\n        out = self.meanfc(ro_feats)\n        \n        # bound output\n        out = self.apply_scaling(out)\n        \n        return out\n    \n    def apply_scaling(self, x):\n        # Apply scaling based on min_values and max_values\n        x = self.min_values + (self.max_values - self.min_values) * self.sigmoid(x)\n        return x\n    \nclass LearnableParams(nn.Module):\n    \"\"\"doesn't use data signals\"\"\"\n\n    def __init__(self, num_params, device, scale_output=\"abm-covid\"):\n        super().__init__()\n        self.device = device\n        self.learnable_params = nn.Parameter(jnp.rand(num_params, device=self.device))\n        self.min_values = jnp.array(MIN_VAL_PARAMS[scale_output], device=self.device)\n        self.max_values = jnp.array(MAX_VAL_PARAMS[scale_output], device=self.device)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self):\n        out = self.learnable_params\n        \"\"\" bound output \"\"\"\n        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n        return out\n    \ndef normal(x, mu, sigma_sq):\n    a = (-1 * (jnp.asarray(x) - mu).pow(2) / (2 * sigma_sq)).exp()\n    b = 1 / (2 * sigma_sq * pi.expand_as(sigma_sq)).sqrt()\n    return a * b\n\ndef save_model(model, file_name, disease, region, week):\n    PATH = os.path.join(SAVE_MODEL_PATH, disease, region)\n    if not os.path.exists(PATH):\n        os.makedirs(PATH)\n    jnp.savez_compressed(PATH + \"/\" + file_name + \" \" + week + \".npz\", **model.params)\n\ndef load_model(model, file_name, disease, region, week, device):\n    PATH = os.path.join(SAVE_MODEL_PATH, disease, region)\n    params = jnp.load(PATH + \"/\" + file_name + \" \" + week + \".npz\")\n    model.params = params\n    return model\n\ndef param_model_forward(param_model, params, x, meta):\n    # get R0 from county network\n    if params[\"model_name\"].startswith(\"GradABM-time-varying\"):\n        action_value = param_model.forward(x, meta) # time-varying\n    elif params[\"model_name\"] == \"ABM-expert\":\n        if params['disease'] == \"COVID\":\n            action_value = jnp.array([2.5, 0.02, 0.5]) # from CDC, for COVID -- previous I0 was 0.01\n    if params['disease'] == \"Flu\":\n        action_value = jnp.array([1.3, 1.0]) # from CDC, for COVID\n        action_value = jnp.repeat(action_value, meta.shape[0], 0)\n    elif \"ABM-pred-correction\" in params[\"model_name\"]: # same as SEIRM-static, but get\n        action_value = param_model.forward()\n        if params['disease'] == \"COVID\":\n            # NOTE: to fix, beta/gamma is for SIR, maybe not the same for SEIRM\n            beta = action_value[0]\n            gamma = action_value[2]\n            mu = action_value[3] # mortality rate\n            initial_infections_percentage = action_value[4]\n            action_value = jnp.stack(\n            [beta / (gamma + mu), mu, initial_infections_percentage]\n            )\n        elif params['disease'] == \"Flu\":\n            beta = action_value[0]\n            # D = action_value[:,1]\n            D = 3.5\n            initial_infections_percentage = action_value[1]\n            action_value = jnp.stack([beta * D, initial_infections_percentage])\n            action_value = action_value.reshape(1, -1) # make sure it's 2d\n            print(\"R0 ABM-pred-correction\", action_value)\n    elif \"GradABM-learnable-params\" in params[\"model_name\"]:\n        action_value = param_model.forward()\n        action_value = jnp.repeat(action_value, meta.shape[0], 0)\n    else:\n        raise ValueError(\"model name not valid\")\n    return action_value\n\ndef build_param_model(params, metas_train_dim, X_train_dim, device, CUSTOM_INIT=True):\n    # get param dimension for ODE\n    if params['disease'] == \"COVID\":\n        ode_param_dim = 5\n        abm_param_dim = 3\n        scale_output_ode = \"seirm\"\n        scale_output_abm = \"abm-covid\"\n    elif params['disease'] == \"Flu\":\n        ode_param_dim = 2\n        abm_param_dim = 2\n        scale_output_ode = \"sirs\"\n        scale_output_abm = \"abm-flu\"\n    training_weeks = params[\"num_steps\"] / 7  # only needed for time-varying\n    training_weeks = int(training_weeks)\n    assert training_weeks == int(training_weeks)\n\n    \"\"\" call constructor of param model depending on the model we want to run\"\"\"\n    if params[\"model_name\"].startswith(\"GradABM-time-varying\"):\n        param_model = CalibNN(\n            metas_train_dim,\n            X_train_dim,\n            torch.device,\n            training_weeks,\n            out_dim=abm_param_dim,\n            scale_output= scale_output_abm,\n        )\n    elif params[\"model_name\"] == \"ABM-expert\":\n        param_model = None\n    elif \"ABM-pred-correction\" in params[\"model_name\"]:\n        # load the param model from ODE\n        # NOTE: currently it uses only R0\n        param_model = LearnableParams(ode_param_dim, device, scale_output_ode).to(\n            device\n        )\n    elif \"GradABM-learnable-params\" in params[\"model_name\"]:\n        param_model = LearnableParams(abm_param_dim, device, scale_output_abm).to(\n            device\n        )\n    else:\n        raise ValueError(\"model name not valid\")\n    return param_model\n\ndef build_simulator(params, devices, counties):\n    \"\"\"Build simulator: ABM or ODE\"\"\"\n    if \"ABM\" in params[\"model_name\"]:\n        if params[\"joint\"]:\n            abm = {}\n            # abm devices are different from the ones for the params model\n            if len(devices) > 1:\n                abm_devices = devices[1:]\n            else:\n                abm_devices = devices\n            num_counties = len(counties)\n            for c in range(num_counties):\n                c_params = copy(params)\n                c_params[\"county_id\"] = counties[c]\n                try:\n                    abm[counties[c]] = GradABM(c_params, abm_devices[c % len(abm_devices)])\n                except FileNotFoundError:\n                    print(f\"Skipping scenario for {counties[c]}. Parameter file not found.\")\n                    \n        else:\n            if len(devices) > 1:\n                abm_device = devices[1]\n            else:\n                abm_device = devices[0]\n            abm = GradABM(params, abm_device)\n    \n    elif \"ODE\" in params[\"model_name\"]:\n        disease = params.get('disease', 'DefaultDisease')\n        if disease == \"COVID\":\n            abm = SEIRM(params, devices[0])\n        elif disease == \"Flu\":\n            abm = SIRS(params, devices[0])\n        else:\n            print(\"Invalid disease specified. Using default simulator.\")\n            abm = DefaultODESimulator(params, devices[0])\n\n    return abm\n\ndef forward_simulator(params, param_values, abm, training_num_steps, counties, devices):\n    \"\"\"Assumes abm contains only one simulator for COVID (one county), and multiple for flu (multiple counties)\"\"\"\n\n    if params[\"joint\"]:\n        num_counties = len(counties)\n        predictions = jnp.empty((num_counties, training_num_steps)).to(devices[0])\n        for time_step in range(training_num_steps):\n            if \"time-varying\" in params[\"model_name\"]:\n                param_t = param_values[:, time_step // 7, :]\n            else:\n                param_t = param_values\n            # go over each abm\n            for c in range(num_counties):\n                try:\n                    model_device = abm[counties[c]].device\n                    population = abm[counties[c]].num_agents\n                    _, pred_t = abm[counties[c]].step(\n                        time_step, param_t[c].to(model_device)\n                    )\n                    predictions[c, time_step] = pred_t.to(devices[0])\n                except KeyError:\n                    print(f\"Skipping county {counties[c]} as simulator object not found.\")\n                    abm = jnp.array([], device='cuda')\n                    model_device = 0\n                    population = 0\n                    _, pred_t = abm[counties[c]].step(\n                        time_step, param_t[c].to(model_device)\n                    )\n                    predictions[c, time_step] = pred_t.to(devices[0])\n                    \n    else:\n        num_counties = 1\n        param_values = param_values.squeeze(0)\n        predictions = []\n        for time_step in range(training_num_steps):\n            if \"time-varying\" in params[\"model_name\"]:\n                param_t = param_values[time_step // 7, :]\n            else:\n                param_t = param_values\n            try:\n                model_device = abm.device\n                _, pred_t = abm.step(time_step, param_t.to(model_device))\n                predictions.append(pred_t.to(devices[0]))\n            except KeyError:\n                print(\"Simulator object not found. Skipping simulation.\")\n        predictions = jnp.stack(predictions, 0).reshape(1, -1)  # num counties, seq len\n\n    # post-process predictions for flu\n    # targets are weekly, so we have to convert from daily to weekly\n    if params['disease'] == \"Flu\":\n        predictions = predictions.reshape(num_counties, -1, 7).sum(2)\n    else:\n        predictions = predictions.reshape(num_counties, -1)\n\n    return predictions.unsqueeze(2)\n\nimport time\nimport os\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom flax.training import train_state\nfrom flax.training import checkpoints\nfrom flax.training.common_utils import shard, get_metrics\nfrom flax.training.checkpoints import restore_checkpoint\nfrom flax.training.common_utils import shard, get_metrics\nfrom jax import random\n\ndef runner(params, devices, verbose):\n    for run_id in range(params[\"num_runs\"]):\n        print(\"Run: \", run_id)\n\n        # set batch size depending on the number of devices\n        batch_size = max(len(devices) - 1, 1)\n\n        # get data loaders and ground truth targets\n        if params['disease'] == \"COVID\":\n            if params[\"joint\"]:\n                (\n                    train_loader,\n                    metas_train_dim,\n                    X_train_dim,\n                    seqlen,\n                ) = fetch_county_data_covid(\n                    params[\"state\"],\n                    \"all\",\n                    pred_week=params[\"pred_week\"],\n                    batch_size=batch_size,\n                    noise_level=params[\"noise_level\"],\n                )\n            else:\n                (\n                    train_loader,\n                    metas_train_dim,\n                    X_train_dim,\n                    seqlen,\n                ) = fetch_county_data_covid(\n                    params[\"state\"],\n                    params[\"county_id\"],\n                    pred_week=params[\"pred_week\"],\n                    batch_size=batch_size,\n                    noise_level=params[\"noise_level\"],\n                )\n            params[\"num_steps\"] = seqlen\n        elif params['disease'] == \"Flu\":\n            if params[\"joint\"]:\n                (\n                    train_loader,\n                    metas_train_dim,\n                    X_train_dim,\n                    seqlen,\n                ) = fetch_county_data_flu(\n                    params[\"state\"],\n                    \"all\",\n                    pred_week=params[\"pred_week\"],\n                    batch_size=batch_size,\n                    noise_level=params[\"noise_level\"],\n                )\n            else:\n                (\n                    train_loader,\n                    metas_train_dim,\n                    X_train_dim,\n                    seqlen,\n                ) = fetch_county_data_flu(\n                    params[\"state\"],\n                    params[\"county_id\"],\n                    pred_week=params[\"pred_week\"],\n                    batch_size=batch_size,\n                    noise_level=params[\"noise_level\"],\n                )\n            params[\"num_steps\"] = seqlen * 7\n\n        # add days ahead to num steps because num steps is used for forward pass of param model\n        training_num_steps = params[\"num_steps\"]\n        params[\"num_steps\"] += DAYS_HEAD\n        param_model = build_param_model(\n            params, metas_train_dim, X_train_dim, devices[0], CUSTOM_INIT=True\n        )\n        # filename to save/load model\n        file_name = \"param_model\" + \"_\" + params[\"model_name\"]\n        # do not train ABM because it uses a different calibration procedure\n        train_flag = (\n            False\n            if params[\"model_name\"].startswith(\"ABM\") or params[\"inference_only\"]\n            else True\n        )\n\n        num_epochs = NUM_EPOCHS_DIFF\n        CLIP = 10\n        if \"learnable-params\" in params[\"model_name\"]:\n            lr = 1e-2  # obtained after tuning\n            num_epochs *= 2\n        else:\n            lr = 1e-4 if params[\"model_name\"].startswith(\"GradABM\") else 1e-4\n\n        \"\"\" step 1: training  \"\"\"\n        if train_flag:\n            assert param_model != None\n            optimizer_def = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, param_model.init_parameters()['params'].values()),\n            lr=lr,\n            weight_decay=0.01)\n            optimizer = optimizer_def.create(param_model)\n            losses = []\n            rng = jax.random.PRNGKey(0)\n            for epi in tqdm(range(num_epochs)):\n                start = time.time()\n                batch_predictions = []\n                if verbose:\n                    print(\"\\n\", \"=\" * 60)\n                    print(\"Epoch: \", epi)\n                epoch_loss = 0\n                print(len(train_loader))\n                for batch, (counties, meta, x, y) in enumerate(train_loader):\n                    print(batch, counties)\n                    # construct abm for each forward pass\n                    abm = build_simulator(copy(params), devices, counties)\n                    # forward pass param model\n                    meta = meta.to(devices[0])\n                    x = x.to(devices[0])\n                    y = y.to(devices[0])\n                    param_values = param_model_forward(param_model, params, x, meta)\n                    if verbose:\n                        if param_values.ndim > 2:\n                            print(param_values[:, [0, -1], :])\n                        else:\n                            print(param_values)\n                    # forward simulator for several time steps\n                    if BENCHMARK_TRAIN:\n                        start_bench = time.time()\n                    predictions = forward_simulator(\n                        params, param_values, abm, training_num_steps, counties, devices\n                    )\n                    if BENCHMARK_TRAIN:\n                        # quit after 1 epoch\n                        print(\"No steps:\", training_num_steps)\n                        print(\"time (s): \", time.time() - start_bench)\n                        quit()\n                    # loss\n                    if verbose:\n                        print(jnp.concatenate((y, predictions), 2))\n                    loss_weight = jnp.ones((len(counties), training_num_steps, 1)).to(\n                        devices[0]\n                    )\n                    loss = jnp.mean((loss_weight * jnp.square(y - predictions)).mean())\n                    grad_fn = jax.value_and_grad(loss_fn)\n                    optimizer, grad = grad_fn(optimizer, x, y)\n                    optimizer = optimizer.apply_gradient(grad)\n                    epoch_loss += jnp.sqrt(loss).item()\n                    print(\"current Loss is - \", epoch_loss)\n                losses.append(epoch_loss / (batch + 1))  # divide by number of batches\n                print(\"total Loss is - \", epoch_loss)\n                if verbose:\n                    print(\"epoch_loss\", epoch_loss)\n\n                \"\"\" save best model \"\"\"\n                if epoch_loss < best_loss:\n                    if params[\"joint\"]:\n                        save_model(\n                            param_model,\n                            file_name,\n                            params['disease'],\n                            \"joint\",\n                            params[\"pred_week\"],\n                        )\n                    else:\n                        save_model(\n                            param_model,\n                            file_name,\n                            params['disease'],\n                            params[\"county_id\"],\n                            params[\"pred_week\"],\n                        )\n                    best_loss = epoch_loss\n\n                print(\"epoch {} time (s): {:.2f}\".format(epi, time.time() - start))\n\n        \"\"\" step 2: inference step  \"\"\"\n        \"\"\" upload best model in inference \"\"\"\n        param_model = None\n        abm = None\n        param_model = build_param_model(\n            copy(params), metas_train_dim, X_train_dim, devices[0], CUSTOM_INIT=True\n        )\n        if not params[\"model_name\"].startswith(\"ABM\"):\n            # load param model if it is not ABM-expert\n            if params[\"joint\"]:\n                param_model = load_model(\n                    param_model,\n                    file_name,\n                    params['disease'],\n                    \"joint\",\n                    params[\"pred_week\"],\n                    devices[0],\n                )\n            else:\n                param_model = load_model(\n                    param_model,\n                    file_name,\n                    params['disease'],\n                    params[\"county_id\"],\n                    params[\"pred_week\"],\n                    devices[0],\n                )\n        elif \"ABM-pred-correction\" in params[\"model_name\"]:\n            # pred-correction, uses param model from ODE\n            file_name = \"param_model\" + \"_\" + \"DiffODE-learnable-params\"\n            if params[\"noise_level\"] > 0:\n                file_name = (\n                    \"param_model\"\n                    + \"_\"\n                    + \"DiffODE-learnable-params\"\n                    + \"-noise\"\n                    + str(params[\"noise_level\"])\n                )\n            param_model = load_model(\n                param_model,\n                file_name,\n                params['disease'],\n                params[\"county_id\"],\n                params[\"pred_week\"],\n                devices[0],\n            )\n\n        num_step = training_num_steps + DAYS_HEAD\n        batch_predictions = []\n        counties_predicted = []\n        learned_params = []\n        with torch.no_grad():\n            for batch, (counties, meta, x, y) in enumerate(train_loader):\n                # construct abm for each forward pass\n                abm = build_simulator(params, devices, counties)\n                # forward pass param model\n                meta = meta.to(devices[0])\n                x = x.to(devices[0])\n                param_values = param_model_forward(param_model, params, x, meta)\n                # forward simulator for several time steps\n                preds = forward_simulator(\n                    params, param_values, abm, num_step, counties, devices\n                )\n                batch_predictions.append(preds)\n                counties_predicted.extend(counties)\n                learned_params.extend(np.array(param_values.cpu().detach()))\n        predictions = torch.cat(batch_predictions, axis=0)\n        # we only care about the last predictions\n        # predictions are weekly, so we only care about the last 4\n        if params['disease'] == \"Flu\":\n            predictions = predictions.squeeze(2)[:, -DAYS_HEAD // 7 :]\n        else:\n            predictions = predictions.squeeze(2)[:, -DAYS_HEAD:]\n        \"\"\" remove grad \"\"\"\n        predictions = predictions.cpu().detach()\n\n        \"\"\" release memory \"\"\"\n        param_model = None\n        abm = None\n        torch.cuda.empty_cache()\n\n        \"\"\" plot losses \"\"\"\n        # only if trained\n        if train_flag:\n            disease = params['disease']\n            if params[\"joint\"]:\n                FIGPATH = f\"/kaggle/working/Figures/{disease}/joint/\"\n            else:\n                county_id = params[\"county_id\"]\n                FIGPATH = f\"/kaggle/working/Figures/{disease}/{county_id}/\"\n            if not os.path.exists(FIGPATH):\n                os.makedirs(FIGPATH)\n            sns.set()\n            fig = plt.figure()\n            ax = fig.add_subplot(1, 1, 1)\n            ax.plot(losses)\n            pred_week = params[\"pred_week\"]\n            fig.savefig(FIGPATH + f\"/losses_{pred_week}.png\")\n        print(\"-\" * 60)\n        return counties_predicted, np.array(predictions), learned_params\n\ndef train_step(optimizer, batch):\n    inputs, targets = batch\n\n    def loss_fn(params):\n        # Compute your loss function\n        predictions = model.apply(params, inputs)\n        loss = your_loss_function(predictions, targets)  # Replace with your actual loss function\n        return loss\n\n    grad_fn = jax.value_and_grad(loss_fn)\n    loss, grad = grad_fn(optimizer.target)\n    optimizer = optimizer.apply_gradient(grad)\n    metrics = {\"loss\": loss}\n\n    return optimizer, metrics\n\ndef train_predict(args):\n    # Setting seed\n    print(\"=\" * 60)\n    if args.joint:\n        print(f\"state {args.state} week {args.pred_week}\")\n    else:\n        print(f\"county {args.county_id} week {args.pred_week}\")\n    print(\"Seed used for python random, numpy, and torch is {}\".format(args.seed))\n#     random.seed(args.seed)\n    np.random.seed(args.seed)\n    jax.random.PRNGKey(args.seed)\n\n    params = {}\n    params[\"seed\"] = args.seed\n    params[\"num_runs\"] = args.num_runs\n    params['disease'] = args.disease\n    params[\"pred_week\"] = args.pred_week\n    params[\"joint\"] = args.joint\n    params[\"inference_only\"] = args.inference_only\n    params[\"noise_level\"] = args.noise  # for robustness experiments\n    # state\n    params[\"state\"] = args.state\n    if params[\"joint\"]:\n        # verify it is a state\n        assert params[\"state\"] in states\n    else:\n        params[\"county_id\"] = args.county_id\n        # verify county belongs to state\n        assert params[\"county_id\"] in counties[params[\"state\"]]\n    params[\"model_name\"] = args.model_name\n\n    devices = jax.devices(backend = None)\n#     if args.dev == [\"cpu\"]:\n#         devices = [jax.devices(\"cpu\")[0]]\n#     else:\n#         devices = [jax.devices(f\"cuda:{i}\")[0] for i in args.dev]\n\n    print(\"devices used:\", devices)\n    verbose = False\n    # Get the learned parameters and make predictions\n    counties_predicted, predictions, learned_params = runner(params, devices, verbose)\n\n    return counties_predicted, predictions, learned_params","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:29:55.329683Z","iopub.execute_input":"2023-05-20T12:29:55.330082Z","iopub.status.idle":"2023-05-20T12:29:55.434671Z","shell.execute_reply.started":"2023-05-20T12:29:55.330050Z","shell.execute_reply":"2023-05-20T12:29:55.433722Z"},"trusted":true},"execution_count":255,"outputs":[{"name":"stdout","text":"---- MAIN IMPORTS SUCCESSFUL -----\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Runner","metadata":{}},{"cell_type":"code","source":"def save_predictions(\n    disease: str,\n    model_name: str,\n    region: str,\n    pred_week: str,\n    death_predictions: np.ndarray,\n):\n    \"\"\"\n    Given an array w/ predictions, save as csv\n    \"\"\"\n    data = np.array([np.arange(len(death_predictions)) + 1, death_predictions])\n    if disease == \"COVID\":\n        df = pd.DataFrame(data.transpose(), columns=[\"k_ahead\", \"deaths\"])\n    elif disease == \"Flu\":\n        df = pd.DataFrame(data.transpose(), columns=[\"k_ahead\", \"ili\"])\n    else:\n        raise ValueError(\"disease must be COVID or Flu\")\n    df[\"k_ahead\"] = df[\"k_ahead\"].astype(\"int8\")\n    path = f\"./Results/{disease}/{region}/\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file_name = f\"preds_{model_name}_{pred_week}.csv\"\n    df.to_csv(os.path.join(path, file_name), index=False)\n\n\ndef save_params(\n    disease: str,\n    model_name: str,\n    region: str,\n    pred_week: str,\n    param_values: np.ndarray,\n):\n    \"\"\"\n    Given an array w/ predictions, save as csv\n    \"\"\"\n    path = f\"./Results/{disease}/{region}/\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n    file_name = f\"params_{model_name}_{pred_week}.csv\"\n    np.savetxt(os.path.join(path, file_name), param_values, delimiter=\",\")","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:29:55.447717Z","iopub.execute_input":"2023-05-20T12:29:55.448403Z","iopub.status.idle":"2023-05-20T12:29:55.459320Z","shell.execute_reply.started":"2023-05-20T12:29:55.448366Z","shell.execute_reply":"2023-05-20T12:29:55.458382Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser(description=\"GradABM for COVID-19 and Flu.\")\nparser.add_argument(\"-m\", \"--model_name\", help=\"Model name.\", default=\"GradABM-time-varying\")\nparser.add_argument(\n    \"-di\", \"--disease\", help=\"Disease: COVID or Flu.\", default=\"COVID\"\n)\nparser.add_argument(\n    \"-s\",\n    \"--seed\",\n    type=int,\n    help=\"Seed for python random, numpy and torch\",\n    default=6666,\n)\nparser.add_argument(\"-n\", \"--num_runs\", type=int, help=\"Number of runs\", default=1)\nparser.add_argument(\"-st\", \"--state\", help=\"State to predict\", default=\"MA\")\nparser.add_argument(\n    \"-c\",\n    \"--county_id\",\n    help=\"County to predict, only when not using joint training\",\n    default=\"25001\",\n)\nparser.add_argument(\n    \"-d\",\n    \"--dev\",\n    nargs=\"+\",\n    type=str,\n    default=\"0\",\n    help=\"Device number to use. Put list for multiple.\",\n)\nparser.add_argument(\n    \"-ew\",\n    \"--pred_ew\",\n    type=str,\n    default=\"202021\",\n    help=\"Prediction week in CDC format\",\n)\nparser.add_argument(\n    \"-j\", \"--joint\", action=\"store_true\", help=\"Train all counties jointly\"\n)\nparser.add_argument(\n    \"-i\",\n    \"--inference_only\",\n    action=\"store_true\",\n    help=\"Will not train if True, inference only\",\n)\nparser.add_argument(\n    \"-no\",\n    \"--noise\",\n    type=int,\n    help=\"Noise level for robustness experiments\",\n    default=0,\n)\nparser.add_argument(\n    \"-f\",\n    \"--results_file_postfix\",\n    help=\"Postfix to be appended to output dir for ease of interpretation\",\n    default=\"\",\n)\nparser.set_defaults(joint=True)  # make true when removing no joint\nparser.set_defaults(inference_only=False)  # make true when removing no joint\nargs = parser.parse_args()\n\n# get list of epiweeks for iteration\ndisease = args.disease\nmodel_name = args.model_name\npred_ew = Week.fromstring(args.pred_ew)\n\ndef run_all_weeks(args):\n    args.pred_week = pred_ew.cdcformat()\n    try:\n        counties_predicted, predictions, learned_params = train_predict(args)\n        num_counties = len(counties_predicted)\n        for c in tqdm(range(num_counties)):\n            save_predictions(\n                disease,\n                model_name,\n                counties_predicted[c],\n                str(pred_ew),\n                predictions[c, :],\n            )\n            save_params(\n                disease,\n                model_name,\n                counties_predicted[c],\n                str(pred_ew),\n                learned_params[c],\n            )\n    except Exception as e:\n        print(\n            f\"exception: did not work for {args.state} week {pred_ew}: \"\n            + str(e)\n            + \"\\n\"\n        )\n        traceback.print_exc()","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:29:55.502921Z","iopub.execute_input":"2023-05-20T12:29:55.503950Z","iopub.status.idle":"2023-05-20T12:29:55.519596Z","shell.execute_reply.started":"2023-05-20T12:29:55.503917Z","shell.execute_reply":"2023-05-20T12:29:55.518620Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"run_all_weeks(copy(args))","metadata":{"execution":{"iopub.status.busy":"2023-05-20T12:29:55.521461Z","iopub.execute_input":"2023-05-20T12:29:55.521818Z","iopub.status.idle":"2023-05-20T12:29:57.923260Z","shell.execute_reply.started":"2023-05-20T12:29:55.521786Z","shell.execute_reply":"2023-05-20T12:29:57.922306Z"},"trusted":true},"execution_count":258,"outputs":[{"name":"stdout","text":"============================================================\nstate MA week 202021\nSeed used for python random, numpy, and torch is 6666\ndevices used: [StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]\nRun:  0\nexception: did not work for MA week 202021: \"CalibNN\" object has no attribute \"init_parameters\". If \"init_parameters\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_31/318974099.py\", line 70, in run_all_weeks\n    counties_predicted, predictions, learned_params = train_predict(args)\n  File \"/tmp/ipykernel_31/22302166.py\", line 709, in train_predict\n    counties_predicted, predictions, learned_params = runner(params, devices, verbose)\n  File \"/tmp/ipykernel_31/22302166.py\", line 474, in runner\n    filter(lambda p: p.requires_grad, param_model.init_parameters()['params'].values()),\n  File \"/opt/conda/lib/python3.10/site-packages/flax/linen/module.py\", line 937, in __getattr__\n    raise AttributeError(msg)\nAttributeError: \"CalibNN\" object has no attribute \"init_parameters\". If \"init_parameters\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}