{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from epiweeks import Week, Year\n",
    "import pandas as pd\n",
    "global N\n",
    "import os\n",
    "dtype = torch.float\n",
    "WEEKS_AHEAD = 4\n",
    "PAD_VALUE = -999\n",
    "DAYS_IN_WEEK = 7\n",
    "NOISE_LEVELS_FLU = [0.15, 0.25, 0.50, 0.75]\n",
    "NOISE_LEVELS_COVID = [0.5, 1.0, 1.5, 2.0]\n",
    "# daily death data\n",
    "\"\"\" County and State Data Processing network\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import pdb\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "\"\"\"For training experiments: freeze values of all params except R0\"\"\"\n",
    "\"\"\" Added with sparse logic \"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from abc import ABC, abstractmethod\n",
    "from scipy.stats import gamma\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "INITIAL_INFECTED_RATIO = 0.5\n",
    "INFINITY_TIME = np.inf  # really large value outside the bounds of simulation steps\n",
    "USE_SPARSE = False\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from copy import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "from epiweeks import Week\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import traceback\n",
    "from copy import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"./Data/Processed/covid_data.csv\"\n",
    "county_datapath = f\"./Data/Processed/county_data.csv\"\n",
    "datapath_flu_hhs = \"./Data/Processed/flu_region_data.csv\"\n",
    "datapath_flu_state = \"./Data/Processed/flu_state_data.csv\"\n",
    "population_path = \"./Data/table_population.csv\"\n",
    "EW_START_DATA = \"202012\"\n",
    "EW_START_DATA_FLU = \"201740\"  # for convenience\n",
    "\n",
    "# Select signals COVID\n",
    "macro_features = [\n",
    "    \"retail_and_recreation_percent_change_from_baseline\",\n",
    "    \"grocery_and_pharmacy_percent_change_from_baseline\",\n",
    "    \"parks_percent_change_from_baseline\",\n",
    "    \"transit_stations_percent_change_from_baseline\",\n",
    "    \"workplaces_percent_change_from_baseline\",\n",
    "    \"residential_percent_change_from_baseline\",\n",
    "    \"apple_mobility\",\n",
    "    \"death_jhu_incidence\",\n",
    "    \"positiveIncr\",\n",
    "]\n",
    "\n",
    "# Select signals Flu\n",
    "include_cols = [\n",
    "    \"symptom:Fever\",\n",
    "    \"symptom:Low-grade fever\",\n",
    "    \"symptom:Cough\",\n",
    "    \"symptom:Sore throat\",\n",
    "    \"symptom:Headache\",\n",
    "    \"symptom:Fatigue\",\n",
    "    \"symptom:Vomiting\",\n",
    "    \"symptom:Diarrhea\",\n",
    "    \"symptom:Shortness of breath\",\n",
    "    \"symptom:Chest pain\",\n",
    "    \"symptom:Dizziness\",\n",
    "    \"symptom:Confusion\",\n",
    "    \"symptom:Generalized tonicâ€“clonic seizure\",\n",
    "    \"symptom:Weakness\",\n",
    "]\n",
    "\n",
    "states = [\n",
    "    \"AL\",\n",
    "    \"AK\",\n",
    "    \"AZ\",\n",
    "    \"AR\",\n",
    "    \"CA\",\n",
    "    \"CO\",\n",
    "    \"CT\",\n",
    "    \"DE\",\n",
    "    \"DC\",\n",
    "    \"FL\",\n",
    "    \"GA\",\n",
    "    \"ID\",\n",
    "    \"IL\",\n",
    "    \"IN\",\n",
    "    \"IA\",\n",
    "    \"KS\",\n",
    "    \"KY\",\n",
    "    \"LA\",\n",
    "    \"ME\",\n",
    "    \"MD\",\n",
    "    \"MA\",\n",
    "    \"MI\",\n",
    "    \"MN\",\n",
    "    \"MS\",\n",
    "    \"MO\",\n",
    "    \"MT\",\n",
    "    \"NE\",\n",
    "    \"NV\",\n",
    "    \"NH\",\n",
    "    \"NJ\",\n",
    "    \"NM\",\n",
    "    \"NY\",\n",
    "    \"NC\",\n",
    "    \"ND\",\n",
    "    \"OH\",\n",
    "    \"OK\",\n",
    "    \"OR\",\n",
    "    \"PA\",\n",
    "    \"RI\",\n",
    "    \"SC\",\n",
    "    \"SD\",\n",
    "    \"TN\",\n",
    "    \"TX\",\n",
    "    \"UT\",\n",
    "    \"VT\",\n",
    "    \"VA\",\n",
    "    \"WA\",\n",
    "    \"WV\",\n",
    "    \"WI\",\n",
    "    \"WY\",\n",
    "    \"X\",\n",
    "]\n",
    "\n",
    "counties = {\n",
    "    \"MA\": [\n",
    "        \"25001\",\n",
    "        \"25003\",\n",
    "        \"25005\",\n",
    "        \"25009\",\n",
    "        \"25011\",\n",
    "        \"25013\",\n",
    "        \"25015\",\n",
    "        \"25021\",\n",
    "        \"25023\",\n",
    "        \"25027\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "########################################################\n",
    "#           helpers\n",
    "########################################################\n",
    "\n",
    "\n",
    "def convert_to_epiweek(x):\n",
    "    return Week.fromstring(str(x))\n",
    "\n",
    "\n",
    "def get_epiweeks_list(start_ew, end_ew):\n",
    "    \"\"\"\n",
    "    returns list of epiweeks objects between start_ew and end_ew (inclusive)\n",
    "    this is useful for iterating through these weeks\n",
    "    \"\"\"\n",
    "    if type(start_ew) == str:\n",
    "        start_ew = convert_to_epiweek(start_ew)\n",
    "    if type(end_ew) == str:\n",
    "        end_ew = convert_to_epiweek(end_ew)\n",
    "    iter_weeks = (\n",
    "        list(Year(2017).iterweeks())\n",
    "        + list(Year(2018).iterweeks())\n",
    "        + list(Year(2019).iterweeks())\n",
    "        + list(Year(2020).iterweeks())\n",
    "        + list(Year(2021).iterweeks())\n",
    "    )\n",
    "    idx_start = iter_weeks.index(start_ew)\n",
    "    idx_end = iter_weeks.index(end_ew)\n",
    "    return iter_weeks[idx_start : idx_end + 1]\n",
    "\n",
    "\n",
    "def create_window_seqs(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    min_sequence_length: int,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates windows of fixed size with appended zeros\n",
    "    @param X: features\n",
    "    @param y: targets, in synchrony with features (i.e. x[t] and y[t] correspond to the same time)\n",
    "    \"\"\"\n",
    "    # convert to small sequences for training, starting with length 10\n",
    "    seqs = []\n",
    "    targets = []\n",
    "    mask_ys = []\n",
    "\n",
    "    # starts at sequence_length and goes until the end\n",
    "    # for idx in range(min_sequence_length, X.shape[0]+1, 7): # last in range is step\n",
    "    for idx in range(min_sequence_length, X.shape[0] + 1, 1):\n",
    "        # Sequences\n",
    "        seqs.append(torch.from_numpy(X[:idx, :]))\n",
    "        # Targets\n",
    "        y_ = y[:idx]\n",
    "        mask_y = torch.ones(len(y_))\n",
    "        targets.append(torch.from_numpy(y_))\n",
    "        mask_ys.append(mask_y)\n",
    "    seqs = pad_sequence(seqs, batch_first=True, padding_value=0).type(dtype)\n",
    "    ys = pad_sequence(targets, batch_first=True, padding_value=PAD_VALUE).type(dtype)\n",
    "    mask_ys = pad_sequence(mask_ys, batch_first=True, padding_value=0).type(dtype)\n",
    "\n",
    "    return seqs, ys, mask_ys\n",
    "\n",
    "########################################################\n",
    "#           COVID: state/national level data\n",
    "########################################################\n",
    "\n",
    "\n",
    "def load_df(region, ew_start_data, ew_end_data):\n",
    "    \"\"\"load and clean data\"\"\"\n",
    "    df = pd.read_csv(datapath, low_memory=False)\n",
    "    df = df[(df[\"region\"] == region)]\n",
    "    df[\"epiweek\"] = df.loc[:, \"epiweek\"].apply(convert_to_epiweek)\n",
    "    # subset data using init parameters\n",
    "    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n",
    "    df = df.fillna(method=\"ffill\")\n",
    "    df = df.fillna(method=\"backfill\")\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_state_train_data(region, pred_week, ew_start_data=EW_START_DATA):\n",
    "    \"\"\"get processed dataframe of data + target as array\"\"\"\n",
    "    # import data\n",
    "    region = str.upper(region)\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    ew_start_data = convert_to_epiweek(ew_start_data)\n",
    "    df = load_df(region, ew_start_data, pred_week)\n",
    "    # select targets\n",
    "    # targets = df.loc[:,['positiveIncr','death_jhu_incidence']].values\n",
    "    targets = df.loc[:, [\"positiveIncr\"]].values\n",
    "    # now subset based on input ew_start_data\n",
    "    df = df[macro_features]\n",
    "    return df, targets\n",
    "\n",
    "\n",
    "def get_state_test_data(region, pred_week):\n",
    "    \"\"\"\n",
    "    @ param pred_week: prediction week\n",
    "    \"\"\"\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    # import smoothed dataframe\n",
    "    df = load_df(region, pred_week + 1, pred_week + 4)\n",
    "    new_cases = df.loc[:, \"positiveIncr\"].values\n",
    "    new_deaths = df.loc[:, \"death_jhu_incidence\"].values\n",
    "    return new_cases, new_deaths\n",
    "\n",
    "\n",
    "def get_train_targets_all_regions(pred_week):\n",
    "    deaths_all_regions = {}\n",
    "    for region in states:\n",
    "        _, targets = get_state_train_data(region, pred_week)\n",
    "        deaths_all_regions[region] = targets[:, 1]  # index 1 is inc deaths\n",
    "    return deaths_all_regions\n",
    "\n",
    "\n",
    "def get_train_features_all_regions(pred_week):\n",
    "    features_all_regions = {}\n",
    "    for region in states:\n",
    "        df, _ = get_state_train_data(region, pred_week)\n",
    "        features_all_regions[region] = df.to_numpy()\n",
    "    return features_all_regions\n",
    "\n",
    "\n",
    "########################################################\n",
    "#           COVID: county level data\n",
    "# note: to obtain data, use get_features_per_county.ipynb\n",
    "########################################################\n",
    "\n",
    "\n",
    "def load_county_df(county, ew_start_data, ew_end_data):\n",
    "    \"\"\"load and clean data\"\"\"\n",
    "    df = pd.read_csv(county_datapath)\n",
    "    df = df[(df[\"geo_value\"] == int(county))]\n",
    "    from datetime import datetime\n",
    "    from datetime import date\n",
    "\n",
    "    def convert_date_to_epiweek(x):\n",
    "        if isinstance(x, Week):\n",
    "            return x\n",
    "        else:\n",
    "            date = datetime.strptime(x, \"%Y-%m-%d\")\n",
    "            return Week.fromdate(date)\n",
    "\n",
    "    df[\"epiweek\"] = df.loc[:, \"time_value\"].apply(convert_date_to_epiweek)    \n",
    "    # subset data using init parameters\n",
    "    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n",
    "    df = df.fillna(0)  # there are zeros at the beginning\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_county_train_data(\n",
    "    county, pred_week, ew_start_data=EW_START_DATA, noise_level=0\n",
    "):\n",
    "    # import data\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    ew_start_data = convert_to_epiweek(ew_start_data)\n",
    "    df = load_county_df(county, ew_start_data, pred_week)\n",
    "    # select targets\n",
    "    targets = df.loc[:, [\"cases\", \"deaths\"]].values\n",
    "    if noise_level > 0:\n",
    "        # noise_level is an index for your list\n",
    "        noise = NOISE_LEVELS_COVID[noise_level - 1]\n",
    "        std_vals = np.std(targets, axis=0) * noise\n",
    "        noise_dist = np.random.normal(scale=std_vals, size=targets.shape)\n",
    "        noisy_targets = targets + noise_dist\n",
    "        noisy_targets = noisy_targets.astype(\"int32\")\n",
    "        targets = np.maximum(noisy_targets, 0)\n",
    "    df.drop(columns=[\"epiweek\", \"geo_value\", \"time_value\"], inplace=True)\n",
    "    return df, targets\n",
    "\n",
    "\n",
    "def get_county_test_data(county, pred_week):\n",
    "    \"\"\"\n",
    "    @ param pred_week: prediction week\n",
    "    \"\"\"\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    # import smoothed dataframe\n",
    "    df = load_county_df(county, pred_week, pred_week + 4)\n",
    "    new_cases = df.loc[:, \"cases\"].values\n",
    "    new_deaths = df.loc[:, \"deaths\"].values\n",
    "    return new_cases, new_deaths\n",
    "\n",
    "\n",
    "########################################################\n",
    "#           FLU: regional/state/national level data\n",
    "########################################################\n",
    "\n",
    "\n",
    "def load_df_flu(region, ew_start_data, ew_end_data, geo):\n",
    "    \"\"\"load and clean data\"\"\"\n",
    "    if geo == \"hhs\":\n",
    "        datapath = datapath_flu_hhs\n",
    "    elif geo == \"state\":\n",
    "        datapath = datapath_flu_state\n",
    "    else:\n",
    "        raise ValueError(\"geo must be hhs or state\")\n",
    "    df = pd.read_csv(datapath, low_memory=False)\n",
    "\n",
    "    df = df[(df[\"region\"] == region)]\n",
    "    df[\"epiweek\"] = df.loc[:, \"epiweek\"].apply(convert_to_epiweek)\n",
    "    # subset data using init parameters\n",
    "    df = df[(df[\"epiweek\"] <= ew_end_data) & (df[\"epiweek\"] >= ew_start_data)]\n",
    "    df = df.fillna(method=\"ffill\")\n",
    "    df = df.fillna(method=\"backfill\")\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_state_train_data_flu(\n",
    "    region, pred_week, ew_start_data=EW_START_DATA_FLU, geo=\"state\", noise_level=0\n",
    "):\n",
    "    \"\"\"get processed dataframe of data + target as array\"\"\"\n",
    "    # import data\n",
    "    region = str.upper(region)\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    ew_start_data = convert_to_epiweek(ew_start_data)\n",
    "    df = load_df_flu(region, ew_start_data, pred_week, geo)\n",
    "    # select targets\n",
    "    # targets = df[\"ili\"].astype(float).values.reshape(-1, 1) \n",
    "    targets = df[\"ili\"].astype(float).values.reshape(-1,1) # we need this 2d\n",
    "    if noise_level > 0:\n",
    "        # noise_level is an index for your list\n",
    "        noise = NOISE_LEVELS_FLU[noise_level - 1]\n",
    "        NOISE_STD = targets.std() * noise\n",
    "        noise_dist = np.random.normal(loc=0, scale=NOISE_STD, size=targets.shape)\n",
    "        noisy_targets = targets + noise_dist\n",
    "        targets = np.array([max(ix, 0) for ix in noisy_targets])\n",
    "    # now subset based on input ew_start_data\n",
    "    df = df[[\"month\"] + include_cols]\n",
    "    return df, targets\n",
    "\n",
    "\n",
    "def get_state_test_data_flu(region, pred_week, geo=\"state\"):\n",
    "    \"\"\"\n",
    "    @ param pred_week: prediction week\n",
    "    \"\"\"\n",
    "    pred_week = convert_to_epiweek(pred_week)\n",
    "    # import smoothed dataframe\n",
    "    df = load_df_flu(region, pred_week + 1, pred_week + 4, geo)\n",
    "    ili = df.loc[:, \"ili\"].values\n",
    "    return ili\n",
    "\n",
    "\n",
    "def get_dir_from_path_list(path):\n",
    "    outdir = path[0]\n",
    "    if not (os.path.exists(outdir)):\n",
    "        os.makedirs(outdir)\n",
    "    for p in path[1:]:\n",
    "        outdir = os.path.join(outdir, p)\n",
    "        if not (os.path.exists(outdir)):\n",
    "            os.makedirs(outdir)\n",
    "    return outdir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\")\n",
    "dtype = torch.float\n",
    "SMOOTH_WINDOW = 7\n",
    "\n",
    "class TransformerAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that calculates self-attention weights using transformer like attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in=40, value_dim=40, key_dim=40) -> None:\n",
    "        \"\"\"\n",
    "        param dim_in: Dimensionality of input sequence\n",
    "        param value_dim: Dimension of value transform\n",
    "        param key_dim: Dimension of key transform\n",
    "        \"\"\"\n",
    "        super(TransformerAttn, self).__init__()\n",
    "        self.value_layer = nn.Linear(dim_in, value_dim)\n",
    "        self.query_layer = nn.Linear(dim_in, value_dim)\n",
    "        self.key_layer = nn.Linear(dim_in, key_dim)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        \"\"\"\n",
    "        param seq: Sequence in dimension [Seq len, Batch, Hidden size]\n",
    "        \"\"\"\n",
    "        seq_in = seq.transpose(0, 1)\n",
    "        value = self.value_layer(seq_in)\n",
    "        query = self.query_layer(seq_in)\n",
    "        keys = self.key_layer(seq_in)\n",
    "        weights = (value @ query.transpose(1, 2)) / math.sqrt(seq.shape[-1])\n",
    "        weights = torch.softmax(weights, -1)\n",
    "        return (weights @ keys).transpose(1, 0)\n",
    "\n",
    "    def forward_mask(self, seq, mask):\n",
    "        \"\"\"\n",
    "        param seq: Sequence in dimension [Seq len, Batch, Hidden size]\n",
    "        \"\"\"\n",
    "        seq_in = seq.transpose(0, 1)\n",
    "        value = self.value_layer(seq_in)\n",
    "        query = self.query_layer(seq_in)\n",
    "        keys = self.key_layer(seq_in)\n",
    "        weights = (value @ query.transpose(1, 2)) / math.sqrt(seq.shape[-1])\n",
    "        weights = torch.exp(weights)\n",
    "        weights = (weights.transpose(1, 2) * mask.transpose(1, 0)).transpose(1, 2)\n",
    "        weights = weights / (weights.sum(-1, keepdim=True))\n",
    "        return (weights @ keys).transpose(1, 0) * mask\n",
    "\n",
    "\n",
    "class EmbedAttenSeq(nn.Module):\n",
    "    \"\"\"\n",
    "    Module to embed a sequence. Adds Attention modul\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_seq_in: int = 5,\n",
    "        dim_metadata: int = 3,\n",
    "        rnn_out: int = 40,\n",
    "        dim_out: int = 50,\n",
    "        n_layers: int = 1,\n",
    "        bidirectional: bool = False,\n",
    "        attn=TransformerAttn,\n",
    "        dropout=0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        param dim_seq_in: Dimensionality of input vector (no. of age groups)\n",
    "        param dim_out: Dimensionality of output vector\n",
    "        param dim_metadata: Dimensions of metadata for all sequences\n",
    "        param rnn_out: output dimension for rnn\n",
    "        \"\"\"\n",
    "        super(EmbedAttenSeq, self).__init__()\n",
    "\n",
    "        self.dim_seq_in = dim_seq_in\n",
    "        self.dim_metadata = dim_metadata\n",
    "        self.rnn_out = rnn_out\n",
    "        self.dim_out = dim_out\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.dim_seq_in,\n",
    "            hidden_size=self.rnn_out // 2 if self.bidirectional else self.rnn_out,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attn_layer = attn(self.rnn_out, self.rnn_out, self.rnn_out)\n",
    "        self.out_layer = [\n",
    "            nn.Linear(\n",
    "                in_features=self.rnn_out + self.dim_metadata, out_features=self.dim_out\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        ]\n",
    "        self.out_layer = nn.Sequential(*self.out_layer)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.out_layer.apply(init_weights)\n",
    "\n",
    "    def forward_mask(self, seqs, metadata, mask):\n",
    "        # Take last output from GRU\n",
    "        latent_seqs = self.rnn(seqs)[0]\n",
    "        latent_seqs = latent_seqs\n",
    "        latent_seqs = self.attn_layer.forward_mask(latent_seqs, mask)\n",
    "        latent_seqs = latent_seqs.sum(0)\n",
    "        out = self.out_layer(torch.cat([latent_seqs, metadata], dim=1))\n",
    "        return out\n",
    "\n",
    "    def forward(self, seqs, metadata):\n",
    "        # Take last output from GRU\n",
    "        latent_seqs, encoder_hidden = self.rnn(seqs)\n",
    "        latent_seqs = self.attn_layer(latent_seqs).sum(0)\n",
    "        out = self.out_layer(torch.cat([latent_seqs, metadata], dim=1))\n",
    "        return out, encoder_hidden\n",
    "\n",
    "\n",
    "class DecodeSeq(nn.Module):\n",
    "    \"\"\"\n",
    "    Module to embed a sequence. Adds Attention modul\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_seq_in: int = 5,\n",
    "        dim_metadata: int = 3,\n",
    "        rnn_out: int = 40,\n",
    "        dim_out: int = 5,\n",
    "        n_layers: int = 1,\n",
    "        bidirectional: bool = False,\n",
    "        dropout=0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        param dim_seq_in: Dimensionality of input vector (no. of age groups)\n",
    "        param dim_out: Dimensionality of output vector\n",
    "        param dim_metadata: Dimensions of metadata for all sequences\n",
    "        param rnn_out: output dimension for rnn\n",
    "        \"\"\"\n",
    "        super(DecodeSeq, self).__init__()\n",
    "\n",
    "        self.dim_seq_in = dim_seq_in\n",
    "        self.dim_metadata = dim_metadata\n",
    "        self.rnn_out = rnn_out\n",
    "        self.dim_out = dim_out\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.act_fcn = nn.Tanh()\n",
    "\n",
    "        # to embed input\n",
    "        self.embed_input = nn.Linear(self.dim_seq_in, self.rnn_out)\n",
    "\n",
    "        # to combine input and context\n",
    "        self.attn_combine = nn.Linear(2 * self.rnn_out, self.rnn_out)\n",
    "\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.rnn_out,\n",
    "            hidden_size=self.rnn_out // 2 if self.bidirectional else self.rnn_out,\n",
    "            bidirectional=bidirectional,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.out_layer = [\n",
    "            nn.Linear(in_features=self.rnn_out, out_features=self.dim_out),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "        ]\n",
    "        self.out_layer = nn.Sequential(*self.out_layer)\n",
    "\n",
    "        # initialize\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.out_layer.apply(init_weights)\n",
    "        self.embed_input.apply(init_weights)\n",
    "        self.attn_combine.apply(init_weights)\n",
    "\n",
    "    def forward(self, Hi_data, encoder_hidden, context):\n",
    "        # Hi_data is scaled time\n",
    "        inputs = Hi_data.transpose(1, 0)\n",
    "        if self.bidirectional:\n",
    "            h0 = torch.cat(encoder_hidden[2:], dim=0)\n",
    "        else:\n",
    "            h0 = torch.stack(encoder_hidden[2:], dim=0).sum(0).unsqueeze(0)\n",
    "        # combine input and context\n",
    "        inputs = self.embed_input(inputs)\n",
    "        # repeat context for each item in sequence\n",
    "        context = context.repeat(inputs.shape[0], 1, 1)\n",
    "        inputs = torch.cat((inputs, context), 2)\n",
    "        inputs = self.attn_combine(inputs)\n",
    "        # Take last output from GRU\n",
    "        latent_seqs = self.rnn(inputs, h0)[0]\n",
    "        latent_seqs = latent_seqs.transpose(1, 0)\n",
    "        latent_seqs = self.out_layer(latent_seqs)\n",
    "        return latent_seqs\n",
    "\n",
    "\n",
    "\"\"\" smooth data with moving average (common with fitting mechanistic models) \"\"\"\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return pd.Series(x).rolling(w, min_periods=1).mean().values\n",
    "\n",
    "\n",
    "\"\"\" Specify which state \"\"\"\n",
    "\n",
    "\n",
    "def fetch_county_data_covid(\n",
    "    state=\"MA\", county_id=\"25005\", pred_week=\"202021\", batch_size=32, noise_level=0\n",
    "):\n",
    "    \"\"\"Import COVID data for counties\"\"\"\n",
    "    np.random.seed(17)\n",
    "\n",
    "    if county_id == \"all\":\n",
    "        all_counties = counties[state]\n",
    "    else:\n",
    "        all_counties = [county_id]\n",
    "\n",
    "    c_seqs = []  # county sequences of features\n",
    "    c_ys = []  # county targets\n",
    "    for county in all_counties:\n",
    "        X_county, y = get_county_train_data(county, pred_week, noise_level=noise_level)\n",
    "        y = moving_average(y[:, 1].ravel(), SMOOTH_WINDOW).reshape(-1, 1)\n",
    "        c_seqs.append(X_county.to_numpy())\n",
    "        c_ys.append(y)\n",
    "    c_seqs = np.array(c_seqs)  # Shape: [regions, time, features]\n",
    "    c_ys = np.array(c_ys)  # Shape: [regions, time, 1]\n",
    "\n",
    "    # Normalize\n",
    "    # One scaler per county\n",
    "    scalers = [StandardScaler() for _ in range(len(all_counties))]\n",
    "    c_seqs_norm = []\n",
    "    for i, scaler in enumerate(scalers):\n",
    "        c_seqs_norm.append(scaler.fit_transform(c_seqs[i]))\n",
    "    c_seqs_norm = np.array(c_seqs_norm)\n",
    "\n",
    "    \"\"\" Create static metadata data for each county \"\"\"\n",
    "\n",
    "    county_idx = {r: i for i, r in enumerate(all_counties)}\n",
    "\n",
    "    def one_hot(idx, dim=len(county_idx)):\n",
    "        ans = np.zeros(dim, dtype=\"float32\")\n",
    "        ans[idx] = 1.0\n",
    "        return ans\n",
    "\n",
    "    metadata = np.array([one_hot(county_idx[r]) for r in all_counties])\n",
    "\n",
    "    \"\"\" Prepare train and validation dataset \"\"\"\n",
    "\n",
    "    min_sequence_length = 20\n",
    "    metas, seqs, y, y_mask = [], [], [], []\n",
    "    for meta, seq, ys in zip(metadata, c_seqs_norm, c_ys):\n",
    "        seq, ys, ys_mask = create_window_seqs(seq, ys, min_sequence_length)\n",
    "        metas.append(meta)\n",
    "        seqs.append(seq[[-1]])\n",
    "        y.append(ys[[-1]])\n",
    "        y_mask.append(ys_mask[[-1]])\n",
    "\n",
    "    all_metas = np.array(metas, dtype=\"float32\")\n",
    "    all_county_seqs = torch.cat(seqs, axis=0)\n",
    "    all_county_ys = torch.cat(y, axis=0)\n",
    "    all_county_y_mask = torch.cat(y_mask, axis=0)\n",
    "\n",
    "    counties_train, metas_train, X_train, y_train, y_mask_train = (\n",
    "        all_counties,\n",
    "        all_metas,\n",
    "        all_county_seqs,\n",
    "        all_county_ys,\n",
    "        all_county_y_mask,\n",
    "    )\n",
    "\n",
    "    train_dataset = SeqData(counties_train, metas_train, X_train, y_train, y_mask_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    assert all_county_seqs.shape[1] == all_county_ys.shape[1]\n",
    "    seqlen = all_county_seqs.shape[1]\n",
    "    return train_loader, metas_train.shape[1], X_train.shape[2], seqlen\n",
    "\n",
    "\n",
    "def fetch_county_data_flu(\n",
    "    state=\"MA\", county_id=\"25005\", pred_week=\"202021\", batch_size=32, noise_level=0\n",
    "):\n",
    "    \"\"\"in flu, our features are state-level and target ILI is only available at state\"\"\"\n",
    "    np.random.seed(17)\n",
    "\n",
    "    \"\"\" Import data for all counties \"\"\"\n",
    "\n",
    "    if county_id == \"all\":\n",
    "        all_counties = counties[state]\n",
    "    else:\n",
    "        all_counties = [county_id]\n",
    "\n",
    "    X_state, y = get_state_train_data_flu(state, pred_week, noise_level=noise_level)\n",
    "    y = moving_average(y.ravel(), SMOOTH_WINDOW).reshape(-1, 1)\n",
    "    c_seqs = []  # county sequences of features\n",
    "    c_ys = []  # county targets\n",
    "    for _ in all_counties:\n",
    "        c_seqs.append(X_state.to_numpy())\n",
    "        c_ys.append(y)\n",
    "    c_seqs = np.array(c_seqs)  # Shape: [regions, time, features]\n",
    "    c_ys = np.array(c_ys)  # Shape: [regions, time, 1]\n",
    "\n",
    "    # Normalize\n",
    "    # One scaler per county\n",
    "    scalers = [StandardScaler() for _ in range(len(all_counties))]\n",
    "    c_seqs_norm = []\n",
    "    for i, scaler in enumerate(scalers):\n",
    "        c_seqs_norm.append(scaler.fit_transform(c_seqs[i]))\n",
    "    c_seqs_norm = np.array(c_seqs_norm)\n",
    "\n",
    "    \"\"\" Create static metadata data for each county \"\"\"\n",
    "\n",
    "    county_idx = {r: i for i, r in enumerate(all_counties)}\n",
    "\n",
    "    def one_hot(idx, dim=len(county_idx)):\n",
    "        ans = np.zeros(dim, dtype=\"float32\")\n",
    "        ans[idx] = 1.0\n",
    "        return ans\n",
    "\n",
    "    metadata = np.array([one_hot(county_idx[r]) for r in all_counties])\n",
    "\n",
    "    \"\"\" Prepare train and validation dataset \"\"\"\n",
    "    min_sequence_length = 5\n",
    "    metas, seqs, y, y_mask = [], [], [], []\n",
    "    for meta, seq, ys in zip(metadata, c_seqs_norm, c_ys):\n",
    "        seq, ys, ys_mask = create_window_seqs(seq, ys, min_sequence_length)\n",
    "        metas.append(meta)\n",
    "        seqs.append(seq[[-1]])\n",
    "        y.append(ys[[-1]])\n",
    "        y_mask.append(ys_mask[[-1]])\n",
    "\n",
    "    all_metas = np.array(metas, dtype=\"float32\")\n",
    "    all_county_seqs = torch.cat(seqs, axis=0)\n",
    "    all_county_ys = torch.cat(y, axis=0)\n",
    "    all_county_y_mask = torch.cat(y_mask, axis=0)\n",
    "\n",
    "    counties_train, metas_train, X_train, y_train, y_mask_train = (\n",
    "        all_counties,\n",
    "        all_metas,\n",
    "        all_county_seqs,\n",
    "        all_county_ys,\n",
    "        all_county_y_mask,\n",
    "    )\n",
    "    train_dataset = SeqData(counties_train, metas_train, X_train, y_train, y_mask_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    assert all_county_seqs.shape[1] == all_county_ys.shape[1]\n",
    "    seqlen = all_county_seqs.shape[1]\n",
    "\n",
    "    return train_loader, metas_train.shape[1], X_train.shape[2], seqlen\n",
    "\n",
    "\n",
    "# dataset class\n",
    "class SeqData(torch.utils.data.Dataset):\n",
    "    def __init__(self, region, meta, X, y, mask_y):\n",
    "        self.region = region\n",
    "        self.meta = meta\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # self.mask_y = mask_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.region[idx], self.meta[idx], self.X[idx, :, :], self.y[idx])\n",
    "\n",
    "\n",
    "class ODE(nn.Module):\n",
    "    def __init__(self, params, device):\n",
    "        super(ODE, self).__init__()\n",
    "        county_id = params[\"county_id\"]\n",
    "        abm_params = f\"Data/{county_id}_generated_params.yaml\"\n",
    "        # Reading params\n",
    "        with open(abm_params, \"r\") as stream:\n",
    "            try:\n",
    "                abm_params = yaml.safe_load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(\"Error in reading parameters file\")\n",
    "                print(exc)\n",
    "        params.update(abm_params)\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.num_agents = self.params[\"num_agents\"]  # Population\n",
    "\n",
    "\n",
    "class SEIRM(ODE):\n",
    "    def __init__(self, params, device):\n",
    "        super().__init__(params, device)\n",
    "\n",
    "    def init_compartments(self, learnable_params):\n",
    "        \"\"\"let's get initial conditions\"\"\"\n",
    "        initial_infections_percentage = learnable_params[\n",
    "            \"initial_infections_percentage\"\n",
    "        ]\n",
    "        initial_conditions = torch.empty((5)).to(self.device)\n",
    "        no_infected = (\n",
    "            initial_infections_percentage / 100\n",
    "        ) * self.num_agents  # 1.0 is ILI\n",
    "        initial_conditions[2] = no_infected\n",
    "        initial_conditions[0] = self.num_agents - no_infected\n",
    "        print(\"initial infected\", no_infected)\n",
    "        self.state = initial_conditions\n",
    "\n",
    "    def step(self, t, values):\n",
    "        \"\"\"\n",
    "        Computes ODE states via equations\n",
    "            state is the array of state value (S,E,I,R,M)\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"beta\": values[0],\n",
    "            \"alpha\": values[1],\n",
    "            \"gamma\": values[2],\n",
    "            \"mu\": values[3],\n",
    "            \"initial_infections_percentage\": values[4],\n",
    "        }\n",
    "        if t == 0:\n",
    "            self.init_compartments(params)\n",
    "        # to make the NN predict lower numbers, we can make its prediction to be N-Susceptible\n",
    "        dSE = params[\"beta\"] * self.state[0] * self.state[2] / self.num_agents\n",
    "        dEI = params[\"alpha\"] * self.state[1]\n",
    "        dIR = params[\"gamma\"] * self.state[2]\n",
    "        dIM = params[\"mu\"] * self.state[2]\n",
    "\n",
    "        dS = -1.0 * dSE\n",
    "        dE = dSE - dEI\n",
    "        dI = dEI - dIR - dIM\n",
    "        dR = dIR\n",
    "        dM = dIM\n",
    "\n",
    "        # concat and reshape to make it rows as obs, cols as states\n",
    "        self.dstate = torch.stack([dS, dE, dI, dR, dM], 0)\n",
    "        NEW_INFECTIONS_TODAY = dEI\n",
    "        NEW_DEATHS_TODAY = dIM\n",
    "        # update state\n",
    "        self.state = self.state + self.dstate\n",
    "\n",
    "        return NEW_INFECTIONS_TODAY, NEW_DEATHS_TODAY\n",
    "\n",
    "\n",
    "class SIRS(ODE):\n",
    "    def __init__(self, params, device):\n",
    "        super().__init__(params, device)\n",
    "\n",
    "    def init_compartments(self, learnable_params):\n",
    "        \"\"\"let's get initial conditions\"\"\"\n",
    "        initial_infections_percentage = learnable_params[\n",
    "            \"initial_infections_percentage\"\n",
    "        ]\n",
    "        initial_conditions = torch.empty((2)).to(self.device)\n",
    "        no_infected = (\n",
    "            initial_infections_percentage / 100\n",
    "        ) * self.num_agents  # 1.0 is ILI\n",
    "        initial_conditions[1] = no_infected\n",
    "        initial_conditions[0] = self.num_agents - no_infected\n",
    "        print(\"initial infected\", no_infected)\n",
    "\n",
    "        self.state = initial_conditions\n",
    "\n",
    "    def step(self, t, values):\n",
    "        \"\"\"\n",
    "        Computes ODE states via equations\n",
    "            state is the array of state value (S,I)\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            \"beta\": values[0],  # contact rate, range: 0-1\n",
    "            \"initial_infections_percentage\": values[1],\n",
    "        }\n",
    "        # set from expertise\n",
    "        params[\"D\"] = 3.5\n",
    "        params[\"L\"] = 2000\n",
    "        if t == 0:\n",
    "            self.init_compartments(params)\n",
    "        dS = (self.num_agents - self.state[0] - self.state[1]) / params[\"L\"] - params[\n",
    "            \"beta\"\n",
    "        ] * self.state[0] * self.state[1] / self.num_agents\n",
    "        dSI = params[\"beta\"] * self.state[0] * self.state[1] / self.num_agents\n",
    "        dI = dSI - self.state[1] / params[\"D\"]\n",
    "\n",
    "        # concat and reshape to make it rows as obs, cols as states\n",
    "        self.dstate = torch.stack([dS, dI], 0)\n",
    "\n",
    "        NEW_INFECTIONS_TODAY = dSI\n",
    "        # ILI is percentage of outpatients with influenza-like illness\n",
    "        # ILI = params['lambda'] * dSI / self.num_agents\n",
    "        # this is what Shaman and Pei do https://github.com/SenPei-CU/Multi-Pathogen_ILI_Forecast/blob/master/code/SIRS_AH.m\n",
    "        ILI = dSI / self.num_agents * 100  # multiply 100 because it is percentage\n",
    "\n",
    "        # update state\n",
    "        self.state = self.state + self.dstate\n",
    "        return NEW_INFECTIONS_TODAY, ILI\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abm-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_INFECTED_RATIO = 0.5\n",
    "INFINITY_TIME = np.inf  # really large value outside the bounds of simulation steps\n",
    "USE_SPARSE = False\n",
    "\"\"\"Utility Modules\"\"\"\n",
    "\n",
    "class LogitRelaxedBernoulli(object):\n",
    "    def __init__(self, logits, temperature=0.3, **kwargs):\n",
    "        self.logits = logits\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def rsample(self):\n",
    "        eps = torch.clamp(\n",
    "            torch.rand(\n",
    "                self.logits.size(), dtype=self.logits.dtype, device=self.logits.device\n",
    "            ),\n",
    "            min=1e-6,\n",
    "            max=1 - 1e-6,\n",
    "        )\n",
    "        y = (self.logits + torch.log(eps) - torch.log(1.0 - eps)) / self.temperature\n",
    "        return y\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        return (\n",
    "            math.log(self.temperature)\n",
    "            - self.temperature * value\n",
    "            + self.logits\n",
    "            - 2 * F.softplus(-self.temperature * value + self.logits)\n",
    "        )\n",
    "\n",
    "\n",
    "def lam(x_i, x_j, edge_attr, t, R, SFSusceptibility, SFInfector, lam_gamma_integrals):\n",
    "    \"\"\"\n",
    "    x_i and x_j are attributes from nodes for all edges in the graph\n",
    "        note: x_j[:, 2].sum() will be higher than current infections because there are some nodes repeated\n",
    "    \"\"\"\n",
    "    S_A_s = SFSusceptibility[x_i[:, 0].long()]\n",
    "    A_s_i = SFInfector[x_j[:, 1].long()]\n",
    "    B_n = edge_attr[1, :]\n",
    "    integrals = torch.zeros_like(B_n)\n",
    "    infected_idx = x_j[:, 2].bool()\n",
    "    infected_times = t - x_j[infected_idx, 3]\n",
    "\n",
    "    integrals[infected_idx] = lam_gamma_integrals[\n",
    "        infected_times.long()\n",
    "    ]  #:,2 is infected index and :,3 is infected time\n",
    "    edge_network_numbers = edge_attr[\n",
    "        0, :\n",
    "    ]  # to account for the fact that mean interactions start at 4th position of x\n",
    "    I_bar = torch.gather(x_i[:, 4:27], 1, edge_network_numbers.view(-1, 1).long()).view(\n",
    "        -1\n",
    "    )  # to account for the fact that mean interactions start at 4th position of x\n",
    "    res = R * S_A_s * A_s_i * B_n * integrals / I_bar  # Edge attribute 1 is B_n\n",
    "\n",
    "    return res.view(-1, 1)\n",
    "\n",
    "\n",
    "class InfectionNetwork(MessagePassing):\n",
    "    \"\"\"Contact network with graph message passing function for disease spread\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, lam, R, SFSusceptibility, SFInfector, lam_gamma_integrals, device\n",
    "    ):\n",
    "        super(InfectionNetwork, self).__init__(aggr=\"add\")\n",
    "        self.lam = lam\n",
    "        self.R = R\n",
    "        self.SFSusceptibility = SFSusceptibility\n",
    "        self.SFInfector = SFInfector\n",
    "        self.lam_gamma_integrals = lam_gamma_integrals\n",
    "        self.device = device\n",
    "\n",
    "    def forward_sparse(self, data, r0_value_trainable):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        t = data.t\n",
    "        # sparse adjacency matrix of inter-agent interactions\n",
    "        S_A_s = self.SFSusceptibility[x[:, 0].long()]\n",
    "        A_s_i = self.SFInfector[x[:, 1].long()]\n",
    "        integrals = torch.zeros_like(S_A_s)\n",
    "        infected_idx = x[:, 2].bool()\n",
    "        infected_times = t - x[infected_idx, 3]\n",
    "        integrals[infected_idx] = self.lam_gamma_integrals[\n",
    "            infected_times.long()\n",
    "        ]  #:,2 is infected index and :,3 is infected time\n",
    "        I_bar = x[:, 4 + 22]  # only info for random network being used in current expts\n",
    "        integral_asi = A_s_i * integrals\n",
    "        sparse_adj = torch.sparse_coo_tensor.from_indices_and_values(\n",
    "            indices=[edge_index[0], edge_index[1]],\n",
    "            values=torch.ones(edge_index.shape[1]),\n",
    "            size=(x.shape[0], x.shape[0]),\n",
    "        ).to(self.device)\n",
    "        sparse_asi = integral_asi.view(-1, 1).to_sparse().to(self.device)\n",
    "        sparse_mult = torch.sparse.mm(sparse_adj, sparse_asi)\n",
    "        dense_mult = sparse_mult.to_dense().view(-1)\n",
    "\n",
    "        # total infection\n",
    "        infection_transmission = (\n",
    "            r0_value_trainable * S_A_s * dense_mult\n",
    "        ) / I_bar  # /I_bar\n",
    "        return infection_transmission.view(1, -1)\n",
    "\n",
    "    def forward(self, data, r0_value_trainable):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        t = data.t\n",
    "        return self.propagate(\n",
    "            edge_index,\n",
    "            x=x,\n",
    "            edge_attr=edge_attr,\n",
    "            t=t,\n",
    "            R=r0_value_trainable,\n",
    "            SFSusceptibility=self.SFSusceptibility,\n",
    "            SFInfector=self.SFInfector,\n",
    "            lam_gamma_integrals=self.lam_gamma_integrals,\n",
    "        )\n",
    "\n",
    "    def message(\n",
    "        self,\n",
    "        x_i,\n",
    "        x_j,\n",
    "        edge_attr,\n",
    "        t,\n",
    "        R,\n",
    "        SFSusceptibility,\n",
    "        SFInfector,\n",
    "        lam_gamma_integrals,\n",
    "    ):\n",
    "        # x_j has shape [E, in_channels]\n",
    "        tmp = self.lam(\n",
    "            x_i, x_j, edge_attr, t, R, SFSusceptibility, SFInfector, lam_gamma_integrals\n",
    "        )  # tmp has shape [E, 2 * in_channels]\n",
    "        return tmp\n",
    "\n",
    "\n",
    "class DiseaseProgression(ABC):\n",
    "    \"\"\"abstract class\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def initialize_variables(self):\n",
    "        \"\"\"initialize tensor variables depending on disease\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_next_stage_times(self):\n",
    "        \"\"\"update time\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_current_stage(self):\n",
    "        \"\"\"update stage\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SEIRMProgression(DiseaseProgression):\n",
    "    \"\"\"SEIRM for COVID-19\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(DiseaseProgression, self).__init__()\n",
    "        # encoding of stages\n",
    "        self.SUSCEPTIBLE_VAR = 0\n",
    "        self.EXPOSED_VAR = 1  # exposed state\n",
    "        self.INFECTED_VAR = 2\n",
    "        self.RECOVERED_VAR = 3\n",
    "        self.MORTALITY_VAR = 4\n",
    "        # default times (only for initialization, later they are learned)\n",
    "        self.EXPOSED_TO_INFECTED_TIME = 3\n",
    "        self.INFECTED_TO_RECOVERED_TIME = 5\n",
    "        # inf time\n",
    "        self.INFINITY_TIME = params[\"num_steps\"] + 1\n",
    "        self.num_agents = params[\"num_agents\"]\n",
    "\n",
    "    def initialize_variables(\n",
    "        self, agents_infected_time, agents_stages, agents_next_stage_times\n",
    "    ):\n",
    "        \"\"\"initialize tensor variables depending on disease\"\"\"\n",
    "        # agents in I have been at least a few days infected as they have been previously in exposed\n",
    "        # assumption that agents in E have been infected 1 day, and agents in I have been infected EXPOSED_TO_INFECTED_TIME days\n",
    "        agents_infected_time[agents_stages == self.EXPOSED_VAR] = -1\n",
    "        agents_infected_time[agents_stages == self.INFECTED_VAR] = (\n",
    "            -1 * self.EXPOSED_TO_INFECTED_TIME\n",
    "        )\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.EXPOSED_VAR\n",
    "        ] = self.EXPOSED_TO_INFECTED_TIME\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.INFECTED_VAR\n",
    "        ] = self.INFECTED_TO_RECOVERED_TIME\n",
    "\n",
    "        return agents_infected_time, agents_next_stage_times\n",
    "\n",
    "    def update_initial_times(\n",
    "        self, learnable_params, agents_stages, agents_next_stage_times\n",
    "    ):\n",
    "        \"\"\"this is for the abm constructor\"\"\"\n",
    "        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n",
    "        exposed_to_infected_time = learnable_params[\"exposed_to_infected_time\"]\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.EXPOSED_VAR\n",
    "        ] = exposed_to_infected_time\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.INFECTED_VAR\n",
    "        ] = infected_to_recovered_time\n",
    "        return agents_next_stage_times\n",
    "\n",
    "    def get_newly_exposed(self, current_stages, potentially_exposed_today):\n",
    "        # we now get the ones that new to exposure\n",
    "        newly_exposed_today = (\n",
    "            current_stages == self.SUSCEPTIBLE_VAR\n",
    "        ) * potentially_exposed_today\n",
    "        return newly_exposed_today\n",
    "\n",
    "    def update_next_stage_times(\n",
    "        self,\n",
    "        learnable_params,\n",
    "        newly_exposed_today,\n",
    "        current_stages,\n",
    "        agents_next_stage_times,\n",
    "        t,\n",
    "    ):\n",
    "        \"\"\"update time\"\"\"\n",
    "        exposed_to_infected_time = learnable_params[\"exposed_to_infected_time\"]\n",
    "        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n",
    "        # for non-exposed\n",
    "        # if S, R, M -> set to default value; if E/I -> update time if your transition time arrived in the current time\n",
    "        new_transition_times = torch.clone(agents_next_stage_times)\n",
    "        curr_stages = torch.clone(current_stages).long()\n",
    "        new_transition_times[\n",
    "            (curr_stages == self.INFECTED_VAR) * (agents_next_stage_times == t)\n",
    "        ] = self.INFINITY_TIME\n",
    "        new_transition_times[\n",
    "            (curr_stages == self.EXPOSED_VAR) * (agents_next_stage_times == t)\n",
    "        ] = (t + infected_to_recovered_time)\n",
    "        return (\n",
    "            newly_exposed_today * (t + 1 + exposed_to_infected_time)\n",
    "            + (1 - newly_exposed_today) * new_transition_times\n",
    "        )\n",
    "\n",
    "    def get_target_variables(\n",
    "        self,\n",
    "        params,\n",
    "        learnable_params,\n",
    "        newly_exposed_today,\n",
    "        current_stages,\n",
    "        agents_next_stage_times,\n",
    "        t,\n",
    "    ):\n",
    "        \"\"\"get recovered (not longer infectious) + targets\"\"\"\n",
    "        mortality_rate = learnable_params[\"mortality_rate\"]\n",
    "        new_death_recovered_today = (\n",
    "            current_stages\n",
    "            * (current_stages == self.INFECTED_VAR)\n",
    "            * (agents_next_stage_times <= t)\n",
    "        ) / self.INFECTED_VAR  # agents when stage changes\n",
    "        # update for newly recovered agents {recovered now}\n",
    "        recovered_dead_now = new_death_recovered_today  # binary bit vector\n",
    "        NEW_DEATHS_TODAY = mortality_rate * new_death_recovered_today.sum()\n",
    "        NEW_INFECTIONS_TODAY = newly_exposed_today.sum()\n",
    "\n",
    "        return recovered_dead_now, NEW_INFECTIONS_TODAY, NEW_DEATHS_TODAY\n",
    "\n",
    "    def update_current_stage(\n",
    "        self, newly_exposed_today, current_stages, agents_next_stage_times, t\n",
    "    ):\n",
    "        \"\"\"progress disease: move agents to different disease stage\"\"\"\n",
    "        transition_to_infected = self.INFECTED_VAR * (\n",
    "            agents_next_stage_times <= t\n",
    "        ) + self.EXPOSED_VAR * (agents_next_stage_times > t)\n",
    "        transition_to_mortality_or_recovered = self.RECOVERED_VAR * (\n",
    "            agents_next_stage_times <= t\n",
    "        ) + self.INFECTED_VAR * (\n",
    "            agents_next_stage_times > t\n",
    "        )  # can be stochastic --> recovered or mortality\n",
    "\n",
    "        # Stage progression for agents NOT newly exposed today'''\n",
    "        # if S -> stay S; if E/I -> see if time to transition has arrived; if R/M -> stay R/M\n",
    "        stage_progression = (\n",
    "            (current_stages == self.SUSCEPTIBLE_VAR) * self.SUSCEPTIBLE_VAR\n",
    "            + (current_stages == self.RECOVERED_VAR) * self.RECOVERED_VAR\n",
    "            + (current_stages == self.MORTALITY_VAR) * self.MORTALITY_VAR\n",
    "            + (current_stages == self.EXPOSED_VAR) * transition_to_infected\n",
    "            + (current_stages == self.INFECTED_VAR)\n",
    "            * transition_to_mortality_or_recovered\n",
    "        )\n",
    "\n",
    "        # update curr stage - if exposed at current step t or not\n",
    "        current_stages = newly_exposed_today * self.EXPOSED_VAR + stage_progression\n",
    "        return current_stages\n",
    "\n",
    "    def init_stages(self, learnable_params, device):\n",
    "        \"\"\"initial_infections_percentage should be between 0.1 to 1\"\"\"\n",
    "        initial_infections_percentage = learnable_params[\n",
    "            \"initial_infections_percentage\"\n",
    "        ]\n",
    "        prob_infected = (initial_infections_percentage / 100) * torch.ones(\n",
    "            (self.num_agents, 1)\n",
    "        ).to(device)\n",
    "        p = torch.hstack((prob_infected, 1 - prob_infected))\n",
    "        cat_logits = torch.log(p + 1e-9)\n",
    "        agents_stages = F.gumbel_softmax(logits=cat_logits, tau=1, hard=True, dim=1)[\n",
    "            :, 0\n",
    "        ]\n",
    "        return agents_stages\n",
    "\n",
    "\n",
    "class SIRSProgression(DiseaseProgression):\n",
    "    \"\"\"SIRS for influenza\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        super(DiseaseProgression, self).__init__()\n",
    "        # encoding of stages\n",
    "        self.SUSCEPTIBLE_VAR = 0\n",
    "        self.INFECTED_VAR = 1\n",
    "        self.RECOVERED_VAR = 2\n",
    "        # default times (only for initialization, later they are learned)\n",
    "        self.INFECTED_TO_RECOVERED_TIME = 5\n",
    "        self.RECOVERED_TO_SUSCEPTIBLE_TIME = 100\n",
    "        # inf time\n",
    "        self.INFINITY_TIME = params[\"num_steps\"] + 1\n",
    "        self.num_agents = params[\"num_agents\"]\n",
    "\n",
    "    def initialize_variables(\n",
    "        self, agents_infected_time, agents_stages, agents_next_stage_times\n",
    "    ):\n",
    "        \"\"\"initialize tensor variables depending on disease\"\"\"\n",
    "        agents_infected_time[agents_stages == self.INFECTED_VAR] = -1\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.INFECTED_VAR\n",
    "        ] = self.INFECTED_TO_RECOVERED_TIME\n",
    "        return agents_infected_time, agents_next_stage_times\n",
    "\n",
    "    def update_initial_times(\n",
    "        self, learnable_params, agents_stages, agents_next_stage_times\n",
    "    ):\n",
    "        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n",
    "        \"\"\" this is for the abm constructor \"\"\"\n",
    "        agents_next_stage_times[\n",
    "            agents_stages == self.INFECTED_VAR\n",
    "        ] = infected_to_recovered_time\n",
    "        return agents_next_stage_times\n",
    "\n",
    "    def get_newly_exposed(self, current_stages, potentially_exposed_today):\n",
    "        # we now get the ones that new to exposure\n",
    "        newly_exposed_today = (\n",
    "            current_stages == self.SUSCEPTIBLE_VAR\n",
    "        ) * potentially_exposed_today\n",
    "        return newly_exposed_today\n",
    "\n",
    "    def update_next_stage_times(\n",
    "        self,\n",
    "        learnable_params,\n",
    "        newly_exposed_today,\n",
    "        current_stages,\n",
    "        agents_next_stage_times,\n",
    "        t,\n",
    "    ):\n",
    "        \"\"\"update time\"\"\"\n",
    "        infected_to_recovered_time = learnable_params[\"infected_to_recovered_time\"]\n",
    "        recovered_to_susceptible_time = learnable_params[\n",
    "            \"recovered_to_susceptible_time\"\n",
    "        ]\n",
    "        # for non-exposed\n",
    "        new_transition_times = torch.clone(agents_next_stage_times)\n",
    "        curr_stages = torch.clone(current_stages).long()\n",
    "        new_transition_times[\n",
    "            (curr_stages == self.INFECTED_VAR) * (agents_next_stage_times == t)\n",
    "        ] = (t + recovered_to_susceptible_time)\n",
    "        new_transition_times[\n",
    "            (curr_stages == self.RECOVERED_VAR) * (agents_next_stage_times == t)\n",
    "        ] = self.INFINITY_TIME  # they go back to susceptible\n",
    "\n",
    "        return (\n",
    "            newly_exposed_today * (t + 1 + infected_to_recovered_time)\n",
    "            + (1 - newly_exposed_today) * new_transition_times\n",
    "        )\n",
    "\n",
    "    def get_target_variables(\n",
    "        self,\n",
    "        params,\n",
    "        learnable_params,\n",
    "        newly_exposed_today,\n",
    "        current_stages,\n",
    "        agents_next_stage_times,\n",
    "        t,\n",
    "    ):\n",
    "        \"\"\"get recovered (not longer infectious) + targets\"\"\"\n",
    "        new_recovered_today = (\n",
    "            current_stages\n",
    "            * (current_stages == self.INFECTED_VAR)\n",
    "            * (agents_next_stage_times <= t)\n",
    "        ) / self.INFECTED_VAR  # agents when stage changes\n",
    "        # get ILI\n",
    "        # ILI =  dSI / self.N * 100 # multiply 100 because it is percentage\n",
    "        # this is what Shaman and Pei do https://github.com/SenPei-CU/Multi-Pathogen_ILI_Forecast/blob/master/code/SIRS_AH.m\n",
    "        ILI = newly_exposed_today.sum() / params[\"num_agents\"] * 100\n",
    "        NEW_INFECTIONS_TODAY = newly_exposed_today.sum()\n",
    "\n",
    "        return new_recovered_today, NEW_INFECTIONS_TODAY, ILI\n",
    "\n",
    "    def update_current_stage(\n",
    "        self, newly_exposed_today, current_stages, agents_next_stage_times, t\n",
    "    ):\n",
    "        \"\"\"progress disease: move agents to different disease stage\"\"\"\n",
    "\n",
    "        transition_to_recovered = self.RECOVERED_VAR * (\n",
    "            agents_next_stage_times <= t\n",
    "        ) + self.INFECTED_VAR * (\n",
    "            agents_next_stage_times > t\n",
    "        )  # can be stochastic --> recovered or mortality\n",
    "        transition_to_susceptible = self.SUSCEPTIBLE_VAR * (\n",
    "            agents_next_stage_times <= t\n",
    "        ) + self.RECOVERED_VAR * (agents_next_stage_times > t)\n",
    "\n",
    "        # Stage progression for agents NOT newly exposed today'''\n",
    "        # if S -> stay S; if E/I -> see if time to transition has arrived; if R/M -> stay R/M\n",
    "        stage_progression = (\n",
    "            (current_stages == self.SUSCEPTIBLE_VAR) * self.SUSCEPTIBLE_VAR\n",
    "            + (current_stages == self.INFECTED_VAR) * transition_to_recovered\n",
    "            + (current_stages == self.RECOVERED_VAR) * transition_to_susceptible\n",
    "        )\n",
    "\n",
    "        current_stages = newly_exposed_today * self.INFECTED_VAR + stage_progression\n",
    "\n",
    "        return current_stages\n",
    "\n",
    "    def init_stages(self, learnable_params, device):\n",
    "        \"\"\"initial_infections_percentage should be between 0.1 to 1\"\"\"\n",
    "\n",
    "        initial_infections_percentage = learnable_params[\n",
    "            \"initial_infections_percentage\"\n",
    "        ]\n",
    "        prob_infected = (initial_infections_percentage / 100) * torch.ones(\n",
    "            (self.num_agents, 1)\n",
    "        ).to(device)\n",
    "        p = torch.hstack((prob_infected, 1 - prob_infected))\n",
    "        cat_logits = torch.log(p + 1e-9)\n",
    "        agents_stages = F.gumbel_softmax(logits=cat_logits, tau=1, hard=True, dim=1)[\n",
    "            :, 0\n",
    "        ]\n",
    "\n",
    "        return agents_stages\n",
    "\n",
    "\n",
    "class GradABM:\n",
    "    def __init__(self, params, device):\n",
    "        county_id = params[\"county_id\"]\n",
    "        abm_params = f\"Data/ABM_parameters/{county_id}_generated_params.yaml\"\n",
    "\n",
    "        # Reading params\n",
    "        with open(abm_params, \"r\") as stream:\n",
    "            try:\n",
    "                abm_params = yaml.safe_load(stream)\n",
    "            except yaml.YAMLError as exc:\n",
    "                print(\"Error in reading parameters file\")\n",
    "                print(exc)\n",
    "\n",
    "        params.update(abm_params)\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.num_agents = self.params[\"num_agents\"]\n",
    "        print(\"Num Agents: \", self.num_agents)\n",
    "\n",
    "        # **********************************************************************************\n",
    "        # Environment state variables\n",
    "        # **********************************************************************************\n",
    "        # **********************************************************************************\n",
    "        # Static\n",
    "        self.agents_ix = (\n",
    "            torch.arange(0, self.params[\"num_agents\"]).long().to(self.device)\n",
    "        )\n",
    "        infile = os.path.join(\n",
    "            self.params[\"output_location\"][\"parent_dir\"],\n",
    "            self.params[\"output_location\"][\"agents_dir\"],\n",
    "            self.params[\"output_location\"][\"agents_outfile\"],\n",
    "        )\n",
    "\n",
    "        agents_df = pd.read_csv(infile)\n",
    "        self.agents_ages = (\n",
    "            torch.tensor(agents_df[\"age_group\"].to_numpy()).long().to(self.device)\n",
    "        )  # age constant in simulation\n",
    "\n",
    "        self.num_networks = 23\n",
    "        self.network_type_dict = {}\n",
    "        self.network_type_dict[\"random\"] = 22\n",
    "        self.network_type_dict_inv = {}\n",
    "        self.network_type_dict_inv[22] = \"random\"\n",
    "\n",
    "        # select disease progression model\n",
    "        if params[\"disease\"] == \"COVID\":\n",
    "            self.DPM = SEIRMProgression(params)\n",
    "        elif params[\"disease\"] == \"Flu\":\n",
    "            self.DPM = SIRSProgression(params)\n",
    "\n",
    "        # Age and Network and Occupation may need to be checked to populate this\n",
    "        self.agents_mean_interactions = 0 * torch.ones(\n",
    "            self.params[\"num_agents\"], self.num_networks\n",
    "        ).to(self.device)\n",
    "\n",
    "        mean_int_ran_df = pd.read_csv(\"Data/Initialization/RandomNetworkParameters.csv\")\n",
    "        mean_int_ran_mu = (\n",
    "            torch.tensor(mean_int_ran_df[\"mu\"].values).float().to(self.device)\n",
    "        )\n",
    "\n",
    "        child_agents = self.agents_ages <= self.params[\"CHILD_Upper_Index\"]\n",
    "        adult_agents = torch.logical_and(\n",
    "            self.agents_ages > self.params[\"CHILD_Upper_Index\"],\n",
    "            self.agents_ages <= self.params[\"ADULT_Upper_Index\"],\n",
    "        ).view(-1)\n",
    "        elderly_agents = self.agents_ages > self.params[\"ADULT_Upper_Index\"]\n",
    "        self.agents_mean_interactions[child_agents.bool(), 22] = mean_int_ran_mu[0]\n",
    "        self.agents_mean_interactions[adult_agents.bool(), 22] = mean_int_ran_mu[1]\n",
    "        self.agents_mean_interactions[elderly_agents.bool(), 22] = mean_int_ran_mu[2]\n",
    "        self.agents_mean_interactions_split = list(\n",
    "            torch.split(self.agents_mean_interactions, 1, dim=1)\n",
    "        )\n",
    "        self.agents_mean_interactions_split = [\n",
    "            a.view(-1) for a in self.agents_mean_interactions_split\n",
    "        ]\n",
    "\n",
    "        self.R = 5.18  # learnable, but the default value\n",
    "        self.R = torch.tensor(self.R).to(self.device)\n",
    "        if self.params[\"disease\"] == \"COVID\":\n",
    "            self.SFSusceptibility = (\n",
    "                torch.tensor([0.35, 0.69, 1.03, 1.03, 1.03, 1.03, 1.27, 1.52, 1.52])\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )\n",
    "            self.SFInfector = (\n",
    "                torch.tensor([0.0, 0.33, 0.72, 0.0, 0.0]).float().to(self.device)\n",
    "            )\n",
    "            self.lam_gamma = {}\n",
    "            self.lam_gamma[\"scale\"] = 5.5\n",
    "            self.lam_gamma[\"rate\"] = 2.14\n",
    "        elif self.params[\"disease\"] == \"Flu\":\n",
    "            # from CDC Table 1, median value: https://www.cdc.gov/flu/about/keyfacts.htm\n",
    "            self.SFSusceptibility = (\n",
    "                torch.tensor([13.2, 7.9, 7.4, 7.4, 7.4, 12.0, 12.0, 3.9, 3.9])\n",
    "                .float()\n",
    "                .to(self.device)\n",
    "            )\n",
    "            self.SFSusceptibility = (\n",
    "                F.softmax(self.SFSusceptibility, 0) * 25\n",
    "            )  # normalize/scale\n",
    "            self.SFInfector = torch.tensor([0.0, 0.72, 0.0]).float().to(self.device)\n",
    "            self.lam_gamma = {}\n",
    "            self.lam_gamma[\"scale\"] = 7.5  # mean is scale/rate, CDC says it's 3-4 days\n",
    "            self.lam_gamma[\"rate\"] = 2.14\n",
    "\n",
    "        self.B_n = {}\n",
    "        self.B_n[\"household\"] = 2\n",
    "        self.B_n[\"occupation\"] = 1\n",
    "        self.B_n[\"random\"] = 1  # 0.25\n",
    "\n",
    "        self.lam_gamma_integrals = self._get_lam_gamma_integrals(\n",
    "            **self.lam_gamma, t=self.params[\"num_steps\"] + 10\n",
    "        )  # add 10 to make sure we cover all\n",
    "        self.lam_gamma_integrals = self.lam_gamma_integrals.to(self.device)\n",
    "        self.net = InfectionNetwork(\n",
    "            lam,\n",
    "            self.R,\n",
    "            self.SFSusceptibility,\n",
    "            self.SFInfector,\n",
    "            self.lam_gamma_integrals,\n",
    "            self.device,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.current_time = 0\n",
    "        # **********************************************************************************\n",
    "        self.all_edgelist, self.all_edgeattr = self.init_interaction_graph(\n",
    "            t=0\n",
    "        )  # get one initial interaction graph\n",
    "\n",
    "    def init_interaction_graph(self, t):\n",
    "        \"\"\"this is Part-1 of Step\"\"\"\n",
    "\n",
    "        infile = os.path.join(\n",
    "            get_dir_from_path_list(\n",
    "                [\n",
    "                    self.params[\"output_location\"][\"parent_dir\"],\n",
    "                    self.params[\"output_location\"][\"networks_dir\"],\n",
    "                    self.params[\"output_location\"][\"random_networks_dir\"],\n",
    "                ]\n",
    "            ),\n",
    "            \"{}.csv\".format(t),\n",
    "        )\n",
    "\n",
    "        random_network_edgelist_forward = (\n",
    "            torch.tensor(pd.read_csv(infile, header=None).to_numpy()).t().long()\n",
    "        )\n",
    "        random_network_edgelist_backward = torch.vstack(\n",
    "            (\n",
    "                random_network_edgelist_forward[1, :],\n",
    "                random_network_edgelist_forward[0, :],\n",
    "            )\n",
    "        )\n",
    "        random_network_edgelist = torch.hstack(\n",
    "            (random_network_edgelist_forward, random_network_edgelist_backward)\n",
    "        )\n",
    "        random_network_edgeattr_type = (\n",
    "            torch.ones(random_network_edgelist.shape[1]).long()\n",
    "            * self.network_type_dict[\"random\"]\n",
    "        )\n",
    "\n",
    "        random_network_edgeattr_B_n = (\n",
    "            torch.ones(random_network_edgelist.shape[1]).float() * self.B_n[\"random\"]\n",
    "        )\n",
    "        random_network_edgeattr = torch.vstack(\n",
    "            (random_network_edgeattr_type, random_network_edgeattr_B_n)\n",
    "        )\n",
    "\n",
    "        all_edgelist = torch.hstack((random_network_edgelist,))\n",
    "        all_edgeattr = torch.hstack((random_network_edgeattr,))\n",
    "\n",
    "        all_edgelist = all_edgelist.to(self.device)\n",
    "        all_edgeattr = all_edgeattr.to(self.device)\n",
    "\n",
    "        return all_edgelist, all_edgeattr\n",
    "\n",
    "    def get_interaction_graph(self, t):\n",
    "        return self.all_edgelist, self.all_edgeattr\n",
    "\n",
    "    def init_state_tensors(self, learnable_params):\n",
    "        \"\"\"Initializing message passing network (currently no trainable parameters here)\"\"\"\n",
    "        # Dynamic\n",
    "        # a.Testing\n",
    "        # b.Quarantine\n",
    "        # c.Infection and Disease\n",
    "        self.current_stages = self.DPM.init_stages(learnable_params, self.device)\n",
    "        self.agents_infected_index = (self.current_stages > 0).to(\n",
    "            self.device\n",
    "        )  # Not susceptible\n",
    "        self.agents_infected_time = (\n",
    "            (self.params[\"num_steps\"] + 1) * torch.ones_like(self.current_stages)\n",
    "        ).to(\n",
    "            self.device\n",
    "        )  # Practically infinite as np.inf gives wrong data type\n",
    "\n",
    "        self.agents_next_stages = -1 * torch.ones_like(self.current_stages).to(\n",
    "            self.device\n",
    "        )\n",
    "        self.agents_next_stage_times = (self.params[\"num_steps\"] + 1) * torch.ones_like(\n",
    "            self.current_stages\n",
    "        ).long().to(\n",
    "            self.device\n",
    "        )  # Practically infinite as np.inf gives wrong data type\n",
    "\n",
    "        # update values depending on the disease\n",
    "        (\n",
    "            self.agents_infected_time,\n",
    "            self.agents_next_stage_times,\n",
    "        ) = self.DPM.initialize_variables(\n",
    "            self.agents_infected_time, self.current_stages, self.agents_next_stage_times\n",
    "        )\n",
    "\n",
    "        self.agents_next_stage_times = self.agents_next_stage_times.float()\n",
    "        self.agents_infected_time = self.agents_infected_time.float()\n",
    "\n",
    "    def step(self, t, param_t):\n",
    "        \"\"\"Send as input: r0_value [hidden state] -> trainable parameters  and t is the time-step of simulation.\"\"\"\n",
    "        # construct dictionary with trainable parameters\n",
    "        learnable_params = {}\n",
    "        if self.params[\"disease\"] == \"COVID\":\n",
    "            learnable_params[\"r0_value\"] = param_t[0]\n",
    "            learnable_params[\"mortality_rate\"] = param_t[1]\n",
    "            learnable_params[\"initial_infections_percentage\"] = param_t[2]\n",
    "            learnable_params[\"exposed_to_infected_time\"] = 3\n",
    "            learnable_params[\"infected_to_recovered_time\"] = 5\n",
    "        elif self.params[\"disease\"] == \"Flu\":\n",
    "            learnable_params[\"r0_value\"] = param_t[0]\n",
    "            learnable_params[\"initial_infections_percentage\"] = param_t[1]\n",
    "            learnable_params[\"infected_to_recovered_time\"] = 3.5\n",
    "            learnable_params[\"recovered_to_susceptible_time\"] = 2000\n",
    "        \"\"\" change params that were set in constructor \"\"\"\n",
    "        if t == 0:\n",
    "            self.init_state_tensors(learnable_params)\n",
    "            self.agents_next_stage_times = self.DPM.update_initial_times(\n",
    "                learnable_params, self.current_stages, self.agents_next_stage_times\n",
    "            )\n",
    "\n",
    "        # t = self.current_time\n",
    "        \"\"\"Steps: i) Get interaction graph, ii) Message Passing of Infection iii) State Evolution \"\"\"\n",
    "\n",
    "        # ******************************************************************************** #\n",
    "        # Part-1. Interaction Graph - Output: EdgeList, EdgeFeatures and NodeFeatures\n",
    "        all_edgelist, all_edgeattr = self.get_interaction_graph(\n",
    "            t\n",
    "        )  # the interaction graphs for GNN at time t\n",
    "        all_nodeattr = torch.stack(\n",
    "            (\n",
    "                self.agents_ages,  # 0\n",
    "                self.current_stages.detach(),  # 1\n",
    "                self.agents_infected_index.to(self.device),  # 2\n",
    "                self.agents_infected_time.to(self.device),  # 3\n",
    "                *self.agents_mean_interactions_split,  # 4 to 26\n",
    "                torch.arange(self.params[\"num_agents\"]).to(\n",
    "                    self.device\n",
    "                ),  # Agent ids (27)\n",
    "            )\n",
    "        ).t()\n",
    "        agents_data = Data(\n",
    "            all_nodeattr,\n",
    "            edge_index=all_edgelist,\n",
    "            edge_attr=all_edgeattr,\n",
    "            t=t,\n",
    "            agents_mean_interactions=self.agents_mean_interactions,\n",
    "        )\n",
    "\n",
    "        # ******************************************************************************** #\n",
    "        # Part-2. Message Passing - Transmission Dynamics + New Infections: {GNN + Variational Inference}\n",
    "        # agent steps: i) collects infection [GNN]; ii) get infected based on total infection collected [Variational Inference]\n",
    "        # message passing: collecting infection from neighbors\n",
    "        lam_t = self.net(agents_data, learnable_params[\"r0_value\"])\n",
    "        prob_not_infected = torch.exp(-lam_t)\n",
    "        p = torch.hstack((1 - prob_not_infected, prob_not_infected))\n",
    "        cat_logits = torch.log(p + 1e-9)\n",
    "        potentially_exposed_today = F.gumbel_softmax(\n",
    "            logits=cat_logits, tau=1, hard=True, dim=1\n",
    "        )[\n",
    "            :, 0\n",
    "        ]  # first column is prob of infections\n",
    "        newly_exposed_today = self.DPM.get_newly_exposed(\n",
    "            self.current_stages, potentially_exposed_today\n",
    "        )\n",
    "\n",
    "        # ******************************************************************************** #\n",
    "        # Part-3. State Evolution -> Progression Dynamics {Deterministic}\n",
    "        # to do here: i) update curr_stage; ii) update next_transition_time\n",
    "        # self.agents_infected_time[t].sum()\n",
    "        # check 2 things: i) got infected_today -> go from S to E; ii) already infected -> update E to I; I to R or M.\n",
    "        # stage_progression, new_death_recovered_today = self.deterministic_stage_transition(self.agents_stages[t,:],\n",
    "\n",
    "        # before updating, get target variables like new deaths or ILI\n",
    "        # also get recovered ones\n",
    "        recovered_dead_now, target1, target2 = self.DPM.get_target_variables(\n",
    "            self.params,\n",
    "            learnable_params,\n",
    "            newly_exposed_today,\n",
    "            self.current_stages,\n",
    "            self.agents_next_stage_times,\n",
    "            t,\n",
    "        )\n",
    "\n",
    "        # get next stages without updating yet the current_stages\n",
    "        next_stages = self.DPM.update_current_stage(\n",
    "            newly_exposed_today, self.current_stages, self.agents_next_stage_times, t\n",
    "        )\n",
    "\n",
    "        # update times with current_stages\n",
    "        self.agents_next_stage_times = self.DPM.update_next_stage_times(\n",
    "            learnable_params,\n",
    "            newly_exposed_today,\n",
    "            self.current_stages,\n",
    "            self.agents_next_stage_times,\n",
    "            t,\n",
    "        )\n",
    "\n",
    "        # safely update current_stages\n",
    "        self.current_stages = next_stages\n",
    "\n",
    "        # update for newly exposed agents {exposed_today}\n",
    "        self.agents_infected_index[newly_exposed_today.bool()] = True\n",
    "        self.agents_infected_time[newly_exposed_today.bool()] = t\n",
    "        # remove recovered from infected indexes\n",
    "        self.agents_infected_index[recovered_dead_now.bool()] = False\n",
    "\n",
    "        # reconcile and return values\n",
    "        self.current_time += 1\n",
    "        return target1, target2\n",
    "\n",
    "    def _get_lam_gamma_integrals(self, scale, rate, t):\n",
    "        b = rate * rate / scale\n",
    "        a = scale / b  # / b\n",
    "        res = [\n",
    "            (\n",
    "                gamma.cdf(t_i, a=a, loc=0, scale=b)\n",
    "                - gamma.cdf(t_i - 1, a=a, loc=0, scale=b)\n",
    "            )\n",
    "            for t_i in range(t)\n",
    "        ]\n",
    "        return torch.tensor(res).float()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_abm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_TRAIN = False\n",
    "NUM_EPOCHS_DIFF = 100\n",
    "print(\"---- MAIN IMPORTS SUCCESSFUL -----\")\n",
    "epsilon = 1e-6\n",
    "\n",
    "MIN_VAL_PARAMS = {\n",
    "    \"abm-covid\": [\n",
    "        1.0,\n",
    "        0.001,\n",
    "        0.01,\n",
    "    ],  # r0, mortality rate, initial_infections_percentage\n",
    "    \"abm-flu\": [1.05, 0.1],  # r0, initial_infections_percentage\n",
    "    \"seirm\": [\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.01,\n",
    "    ],  # beta, alpha, gamma, mu, initial_infections_percentage\n",
    "    \"sirs\": [0.0, 0.1],  # beta, initial_infections_percentage\n",
    "}\n",
    "MAX_VAL_PARAMS = {\n",
    "    \"abm-covid\": [8.0, 0.02, 1.0],\n",
    "    \"abm-flu\": [2.6, 5.0],\n",
    "    \"seirm\": [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    \"sirs\": [1.0, 5.0],\n",
    "}\n",
    "\n",
    "DAYS_HEAD = 4 * 7  # 4 weeks ahead\n",
    "\n",
    "pi = torch.FloatTensor([math.pi])\n",
    "\n",
    "SAVE_MODEL_PATH = \"./Models/\"\n",
    "\n",
    "# neural network predicting parameters of the ABM\n",
    "\n",
    "\n",
    "class CalibNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metas_train_dim,\n",
    "        X_train_dim,\n",
    "        device,\n",
    "        training_weeks,\n",
    "        hidden_dim=32,\n",
    "        out_dim=1,\n",
    "        n_layers=2,\n",
    "        scale_output=\"abm-covid\",\n",
    "        bidirectional=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.training_weeks = training_weeks\n",
    "\n",
    "        \"\"\" tune \"\"\"\n",
    "        hidden_dim = 64\n",
    "        out_layer_dim = 32\n",
    "\n",
    "        self.emb_model = EmbedAttenSeq(\n",
    "            dim_seq_in=X_train_dim,\n",
    "            dim_metadata=metas_train_dim,\n",
    "            rnn_out=hidden_dim,\n",
    "            dim_out=hidden_dim,\n",
    "            n_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.decoder = DecodeSeq(\n",
    "            dim_seq_in=1,\n",
    "            rnn_out=hidden_dim,  # divides by 2 if bidirectional\n",
    "            dim_out=out_layer_dim,\n",
    "            n_layers=1,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        out_layer_width = out_layer_dim\n",
    "        self.out_layer = [\n",
    "            nn.Linear(in_features=out_layer_width, out_features=out_layer_width // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=out_layer_width // 2, out_features=out_dim),\n",
    "        ]\n",
    "        self.out_layer = nn.Sequential(*self.out_layer)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "        self.out_layer.apply(init_weights)\n",
    "        self.min_values = torch.tensor(MIN_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.max_values = torch.tensor(MAX_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, meta):\n",
    "        x_embeds, encoder_hidden = self.emb_model.forward(x.transpose(1, 0), meta)\n",
    "        # create input that will tell the neural network which week it is predicting\n",
    "        # thus, we have one element in the sequence per each week of R0\n",
    "        time_seq = (\n",
    "            torch.arange(1, self.training_weeks + WEEKS_AHEAD + 1)\n",
    "            .repeat(x_embeds.shape[0], 1)\n",
    "            .unsqueeze(2)\n",
    "        )\n",
    "        Hi_data = ((time_seq - time_seq.min()) / (time_seq.max() - time_seq.min())).to(\n",
    "            self.device\n",
    "        )\n",
    "        emb = self.decoder(Hi_data, encoder_hidden, x_embeds)\n",
    "        out = self.out_layer(emb)\n",
    "        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ParamModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        metas_train_dim,\n",
    "        X_train_dim,\n",
    "        device,\n",
    "        hidden_dim=50,\n",
    "        n_layers=2,\n",
    "        out_dim=1,\n",
    "        scale_output=\"abm-covid\",\n",
    "        bidirectional=True,\n",
    "        CUSTOM_INIT=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.emb_model = EmbedAttenSeq(\n",
    "            dim_seq_in=X_train_dim,\n",
    "            dim_metadata=metas_train_dim,\n",
    "            dim_out=hidden_dim,\n",
    "            n_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.layer1 = nn.Linear(in_features=hidden_dim, out_features=20)\n",
    "        # used to bypass the RNN - we want to check what's happening with gradients\n",
    "        self.layer_bypass = nn.Linear(in_features=metas_train_dim, out_features=20)\n",
    "        self.meanfc = nn.Linear(in_features=20, out_features=out_dim, bias=True)\n",
    "        self.min_values = torch.tensor(MIN_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.max_values = torch.tensor(MAX_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        if CUSTOM_INIT:\n",
    "            self.meanfc.bias = torch.nn.Parameter(torch.tensor([1.0]))\n",
    "\n",
    "    def forward(self, x, meta):\n",
    "        x_embeds = self.emb_model.forward(x.transpose(1, 0), meta)\n",
    "        # use embedding for predicting: i) R0 and ii) Cases {for support counties} [FOR LATER]\n",
    "        ro_feats = self.layer1(x_embeds)\n",
    "        ro_feats = nn.ReLU()(ro_feats)\n",
    "        out = self.meanfc(ro_feats)\n",
    "        # else:\n",
    "        \"\"\" bound output \"\"\"\n",
    "        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class LearnableParams(nn.Module):\n",
    "    \"\"\"doesn't use data signals\"\"\"\n",
    "\n",
    "    def __init__(self, num_params, device, scale_output=\"abm-covid\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.learnable_params = nn.Parameter(torch.rand(num_params, device=self.device))\n",
    "        self.min_values = torch.tensor(MIN_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.max_values = torch.tensor(MAX_VAL_PARAMS[scale_output], device=self.device)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self):\n",
    "        out = self.learnable_params\n",
    "        \"\"\" bound output \"\"\"\n",
    "        out = self.min_values + (self.max_values - self.min_values) * self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def normal(x, mu, sigma_sq):\n",
    "    a = (-1 * (Variable(x) - mu).pow(2) / (2 * sigma_sq)).exp()\n",
    "    b = 1 / (2 * sigma_sq * pi.expand_as(sigma_sq)).sqrt()\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def save_model(model, file_name, disease, region, week):\n",
    "    PATH = os.path.join(SAVE_MODEL_PATH, disease, region)\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "    torch.save(model.state_dict(), PATH + \"/\" + file_name + \" \" + week + \".pth\")\n",
    "\n",
    "\n",
    "def load_model(model, file_name, disease, region, week, device):\n",
    "    PATH = os.path.join(SAVE_MODEL_PATH, disease, region)\n",
    "    model.load_state_dict(\n",
    "        torch.load(PATH + \"/\" + file_name + \" \" + week + \".pth\", map_location=device)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def param_model_forward(param_model, params, x, meta):\n",
    "    # get R0 from county network\n",
    "    if params[\"model_name\"].startswith(\"GradABM-time-varying\"):\n",
    "        action_value = param_model.forward(x, meta)  # time-varying\n",
    "    elif params[\"model_name\"] == \"ABM-expert\":\n",
    "        if params[\"disease\"] == \"COVID\":\n",
    "            action_value = torch.tensor(\n",
    "                [2.5, 0.02, 0.5]\n",
    "            )  # from CDC, for COVID -- previous I0 was 0.01\n",
    "        if params[\"disease\"] == \"Flu\":\n",
    "            action_value = torch.tensor([1.3, 1.0])  # from CDC, for COVID\n",
    "        action_value = action_value.repeat((meta.shape[0], 1))\n",
    "    elif \"ABM-pred-correction\" in params[\"model_name\"]:  # same as SEIRM-static, but get\n",
    "        action_value = param_model.forward()\n",
    "        if params[\"disease\"] == \"COVID\":\n",
    "            # NOTE: to fix, beta/gamma is for SIR, maybe not the same for SEIRM\n",
    "            beta = action_value[0]\n",
    "            gamma = action_value[2]\n",
    "            mu = action_value[3]  # mortality rate\n",
    "            initial_infections_percentage = action_value[4]\n",
    "            action_value = torch.stack(\n",
    "                [beta / (gamma + mu), mu, initial_infections_percentage]\n",
    "            )\n",
    "        elif params[\"disease\"] == \"Flu\":\n",
    "            beta = action_value[0]\n",
    "            # D = action_value[:,1]\n",
    "            D = 3.5\n",
    "            initial_infections_percentage = action_value[1]\n",
    "            action_value = torch.stack([beta * D, initial_infections_percentage])\n",
    "        action_value = action_value.reshape(1, -1)  # make sure it's 2d\n",
    "        print(\"R0 ABM-pred-correction\", action_value)\n",
    "    elif \"GradABM-learnable-params\" in params[\"model_name\"]:\n",
    "        action_value = param_model.forward()\n",
    "        action_value = action_value.repeat((meta.shape[0], 1))\n",
    "    else:\n",
    "        raise ValueError(\"model name not valid\")\n",
    "    return action_value\n",
    "\n",
    "\n",
    "def build_param_model(params, metas_train_dim, X_train_dim, device, CUSTOM_INIT=True):\n",
    "    # get param dimension for ODE\n",
    "    if params[\"disease\"] == \"COVID\":\n",
    "        ode_param_dim = 5\n",
    "        abm_param_dim = 3\n",
    "        scale_output_ode = \"seirm\"\n",
    "        scale_output_abm = \"abm-covid\"\n",
    "    elif params[\"disease\"] == \"Flu\":\n",
    "        ode_param_dim = 2\n",
    "        abm_param_dim = 2\n",
    "        scale_output_ode = \"sirs\"\n",
    "        scale_output_abm = \"abm-flu\"\n",
    "    training_weeks = params[\"num_steps\"] / 7  # only needed for time-varying\n",
    "    assert training_weeks == int(training_weeks)\n",
    "\n",
    "    \"\"\" call constructor of param model depending on the model we want to run\"\"\"\n",
    "    if params[\"model_name\"].startswith(\"GradABM-time-varying\"):\n",
    "        param_model = CalibNN(\n",
    "            metas_train_dim,\n",
    "            X_train_dim,\n",
    "            device,\n",
    "            training_weeks,\n",
    "            out_dim=abm_param_dim,\n",
    "            scale_output=scale_output_abm,\n",
    "        ).to(device)\n",
    "    elif params[\"model_name\"] == \"ABM-expert\":\n",
    "        param_model = None\n",
    "    elif \"ABM-pred-correction\" in params[\"model_name\"]:\n",
    "        # load the param model from ODE\n",
    "        # NOTE: currently it uses only R0\n",
    "        param_model = LearnableParams(ode_param_dim, device, scale_output_ode).to(\n",
    "            device\n",
    "        )\n",
    "    elif \"GradABM-learnable-params\" in params[\"model_name\"]:\n",
    "        param_model = LearnableParams(abm_param_dim, device, scale_output_abm).to(\n",
    "            device\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model name not valid\")\n",
    "    return param_model\n",
    "\n",
    "\n",
    "def build_simulator(params, devices, counties):\n",
    "    \"\"\"build simulator: ABM or ODE\"\"\"\n",
    "\n",
    "    if \"ABM\" in params[\"model_name\"]:\n",
    "        if params[\"joint\"]:\n",
    "            abm = {}\n",
    "            # abm devices are different from the ones for the params model\n",
    "            if len(devices) > 1:\n",
    "                abm_devices = devices[1:]\n",
    "            else:\n",
    "                abm_devices = devices\n",
    "            num_counties = len(counties)\n",
    "            for c in range(num_counties):\n",
    "                c_params = copy(params)\n",
    "                c_params[\"county_id\"] = counties[c]\n",
    "                abm[counties[c]] = GradABM(c_params, abm_devices[c % len(abm_devices)])\n",
    "        else:\n",
    "            if len(devices) > 1:\n",
    "                abm_device = devices[1]\n",
    "            else:\n",
    "                abm_device = devices[0]\n",
    "            abm = GradABM(params, abm_device)\n",
    "\n",
    "    elif \"ODE\" in params[\"model_name\"]:\n",
    "        if params[\"disease\"] == \"COVID\":\n",
    "            abm = SEIRM(params, devices[0])\n",
    "        elif params[\"disease\"] == \"Flu\":\n",
    "            abm = SIRS(params, devices[0])\n",
    "\n",
    "    return abm\n",
    "\n",
    "\n",
    "def forward_simulator(params, param_values, abm, training_num_steps, counties, devices):\n",
    "    \"\"\"assumes abm contains only one simulator for covid (one county), and multiple for flu (multiple counties)\"\"\"\n",
    "\n",
    "    if params[\"joint\"]:\n",
    "        num_counties = len(counties)\n",
    "        predictions = torch.empty((num_counties, training_num_steps)).to(devices[0])\n",
    "        for time_step in range(training_num_steps):\n",
    "            if \"time-varying\" in params[\"model_name\"]:\n",
    "                param_t = param_values[:, time_step // 7, :]\n",
    "            else:\n",
    "                param_t = param_values\n",
    "            # go over each abm\n",
    "            for c in range(num_counties):\n",
    "                model_device = abm[counties[c]].device\n",
    "                population = abm[counties[c]].num_agents\n",
    "                _, pred_t = abm[counties[c]].step(\n",
    "                    time_step, param_t[c].to(model_device)\n",
    "                )\n",
    "                predictions[c, time_step] = pred_t.to(devices[0])\n",
    "    else:\n",
    "        num_counties = 1\n",
    "        param_values = param_values.squeeze(0)\n",
    "        predictions = []\n",
    "        for time_step in range(training_num_steps):\n",
    "            if \"time-varying\" in params[\"model_name\"]:\n",
    "                param_t = param_values[time_step // 7, :]\n",
    "            else:\n",
    "                param_t = param_values\n",
    "            model_device = abm.device\n",
    "            _, pred_t = abm.step(time_step, param_t.to(model_device))\n",
    "            predictions.append(pred_t.to(devices[0]))\n",
    "        predictions = torch.stack(predictions, 0).reshape(\n",
    "            1, -1\n",
    "        )  # num counties, seq len\n",
    "\n",
    "    # post-process predictions for flu\n",
    "    # targets are weekly, so we have to convert from daily to weekly\n",
    "    if params[\"disease\"] == \"Flu\":\n",
    "        predictions = predictions.reshape(num_counties, -1, 7).sum(2)\n",
    "    else:\n",
    "        predictions = predictions.reshape(num_counties, -1)\n",
    "\n",
    "    return predictions.unsqueeze(2)\n",
    "\n",
    "\n",
    "def runner(params, devices, verbose):\n",
    "    for run_id in range(params[\"num_runs\"]):\n",
    "        print(\"Run: \", run_id)\n",
    "\n",
    "        # set batch size depending on the number of devices\n",
    "        batch_size = max(len(devices) - 1, 1)\n",
    "\n",
    "        # get data loaders and ground truth targets\n",
    "        if params[\"disease\"] == \"COVID\":\n",
    "            if params[\"joint\"]:\n",
    "                (\n",
    "                    train_loader,\n",
    "                    metas_train_dim,\n",
    "                    X_train_dim,\n",
    "                    seqlen,\n",
    "                ) = fetch_county_data_covid(\n",
    "                    params[\"state\"],\n",
    "                    \"all\",\n",
    "                    pred_week=params[\"pred_week\"],\n",
    "                    batch_size=batch_size,\n",
    "                    noise_level=params[\"noise_level\"],\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    train_loader,\n",
    "                    metas_train_dim,\n",
    "                    X_train_dim,\n",
    "                    seqlen,\n",
    "                ) = fetch_county_data_covid(\n",
    "                    params[\"state\"],\n",
    "                    params[\"county_id\"],\n",
    "                    pred_week=params[\"pred_week\"],\n",
    "                    batch_size=batch_size,\n",
    "                    noise_level=params[\"noise_level\"],\n",
    "                )\n",
    "            params[\"num_steps\"] = seqlen\n",
    "        elif params[\"disease\"] == \"Flu\":\n",
    "            if params[\"joint\"]:\n",
    "                (\n",
    "                    train_loader,\n",
    "                    metas_train_dim,\n",
    "                    X_train_dim,\n",
    "                    seqlen,\n",
    "                ) = fetch_county_data_flu(\n",
    "                    params[\"state\"],\n",
    "                    \"all\",\n",
    "                    pred_week=params[\"pred_week\"],\n",
    "                    batch_size=batch_size,\n",
    "                    noise_level=params[\"noise_level\"],\n",
    "                )\n",
    "            else:\n",
    "                (\n",
    "                    train_loader,\n",
    "                    metas_train_dim,\n",
    "                    X_train_dim,\n",
    "                    seqlen,\n",
    "                ) = fetch_county_data_flu(\n",
    "                    params[\"state\"],\n",
    "                    params[\"county_id\"],\n",
    "                    pred_week=params[\"pred_week\"],\n",
    "                    batch_size=batch_size,\n",
    "                    noise_level=params[\"noise_level\"],\n",
    "                )\n",
    "            params[\"num_steps\"] = seqlen * 7\n",
    "\n",
    "        # add days ahead to num steps because num steps is used for forward pass of param model\n",
    "        training_num_steps = params[\"num_steps\"]\n",
    "        params[\"num_steps\"] += DAYS_HEAD\n",
    "        param_model = build_param_model(\n",
    "            params, metas_train_dim, X_train_dim, devices[0], CUSTOM_INIT=True\n",
    "        )\n",
    "        # filename to save/load model\n",
    "        file_name = \"param_model\" + \"_\" + params[\"model_name\"]\n",
    "        # do not train ABM because it uses a different calibration procedure\n",
    "        train_flag = (\n",
    "            False\n",
    "            if params[\"model_name\"].startswith(\"ABM\") or params[\"inference_only\"]\n",
    "            else True\n",
    "        )\n",
    "\n",
    "        num_epochs = NUM_EPOCHS_DIFF\n",
    "        CLIP = 10\n",
    "        if \"learnable-params\" in params[\"model_name\"]:\n",
    "            lr = 1e-2  # obtained after tuning\n",
    "            num_epochs *= 2\n",
    "        else:\n",
    "            lr = 1e-4 if params[\"model_name\"].startswith(\"GradABM\") else 1e-4\n",
    "\n",
    "        \"\"\" step 1: training  \"\"\"\n",
    "        if train_flag:\n",
    "            assert param_model != None\n",
    "            opt = torch.optim.Adam(\n",
    "                filter(lambda p: p.requires_grad, param_model.parameters()),\n",
    "                lr=lr,\n",
    "                weight_decay=0.01,\n",
    "            )\n",
    "\n",
    "            loss_fcn = torch.nn.MSELoss(reduction=\"none\")\n",
    "            best_loss = np.inf\n",
    "            losses = []\n",
    "            for epi in range(num_epochs):\n",
    "                start = time.time()\n",
    "                batch_predictions = []\n",
    "                if verbose:\n",
    "                    print(\"\\n\", \"=\" * 60)\n",
    "                    print(\"Epoch: \", epi)\n",
    "                epoch_loss = 0\n",
    "                for batch, (counties, meta, x, y) in enumerate(train_loader):\n",
    "                    print(batch, counties)\n",
    "                    # construct abm for each forward pass\n",
    "                    abm = build_simulator(copy(params), devices, counties)\n",
    "                    # forward pass param model\n",
    "                    meta = meta.to(devices[0])\n",
    "                    x = x.to(devices[0])\n",
    "                    y = y.to(devices[0])\n",
    "                    param_values = param_model_forward(param_model, params, x, meta)\n",
    "                    if verbose:\n",
    "                        if param_values.dim() > 2:\n",
    "                            print(param_values[:, [0, -1], :])\n",
    "                        else:\n",
    "                            print(param_values)\n",
    "                    # forward simulator for several time steps\n",
    "                    if BENCHMARK_TRAIN:\n",
    "                        start_bench = time.time()\n",
    "                    predictions = forward_simulator(\n",
    "                        params, param_values, abm, training_num_steps, counties, devices\n",
    "                    )\n",
    "                    if BENCHMARK_TRAIN:\n",
    "                        # quit after 1 epoch\n",
    "                        print(\"No steps:\", training_num_steps)\n",
    "                        print(\"time (s): \", time.time() - start_bench)\n",
    "                        quit()\n",
    "                    # loss\n",
    "                    if verbose:\n",
    "                        print(torch.cat((y, predictions), 2))\n",
    "                    loss_weight = torch.ones((len(counties), training_num_steps, 1)).to(\n",
    "                        devices[0]\n",
    "                    )\n",
    "                    loss = (loss_weight * loss_fcn(y, predictions)).mean()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(param_model.parameters(), CLIP)\n",
    "                    opt.step()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    epoch_loss += torch.sqrt(loss.detach()).item()\n",
    "                losses.append(epoch_loss / (batch + 1))  # divide by number of batches\n",
    "                if verbose:\n",
    "                    print(\"epoch_loss\", epoch_loss)\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    break\n",
    "                \"\"\" save best model \"\"\"\n",
    "                if epoch_loss < best_loss:\n",
    "                    if params[\"joint\"]:\n",
    "                        save_model(\n",
    "                            param_model,\n",
    "                            file_name,\n",
    "                            params[\"disease\"],\n",
    "                            \"joint\",\n",
    "                            params[\"pred_week\"],\n",
    "                        )\n",
    "                    else:\n",
    "                        save_model(\n",
    "                            param_model,\n",
    "                            file_name,\n",
    "                            params[\"disease\"],\n",
    "                            params[\"county_id\"],\n",
    "                            params[\"pred_week\"],\n",
    "                        )\n",
    "                    best_loss = epoch_loss\n",
    "\n",
    "                print(\"epoch {} time (s): {:.2f}\".format(epi, time.time() - start))\n",
    "\n",
    "        \"\"\" step 2: inference step  \"\"\"\n",
    "        \"\"\" upload best model in inference \"\"\"\n",
    "        param_model = None\n",
    "        abm = None\n",
    "        param_model = build_param_model(\n",
    "            copy(params), metas_train_dim, X_train_dim, devices[0], CUSTOM_INIT=True\n",
    "        )\n",
    "        if not params[\"model_name\"].startswith(\"ABM\"):\n",
    "            # load param model if it is not ABM-expert\n",
    "            if params[\"joint\"]:\n",
    "                param_model = load_model(\n",
    "                    param_model,\n",
    "                    file_name,\n",
    "                    params[\"disease\"],\n",
    "                    \"joint\",\n",
    "                    params[\"pred_week\"],\n",
    "                    devices[0],\n",
    "                )\n",
    "            else:\n",
    "                param_model = load_model(\n",
    "                    param_model,\n",
    "                    file_name,\n",
    "                    params[\"disease\"],\n",
    "                    params[\"county_id\"],\n",
    "                    params[\"pred_week\"],\n",
    "                    devices[0],\n",
    "                )\n",
    "        elif \"ABM-pred-correction\" in params[\"model_name\"]:\n",
    "            # pred-correction, uses param model from ODE\n",
    "            file_name = \"param_model\" + \"_\" + \"DiffODE-learnable-params\"\n",
    "            if params[\"noise_level\"] > 0:\n",
    "                file_name = (\n",
    "                    \"param_model\"\n",
    "                    + \"_\"\n",
    "                    + \"DiffODE-learnable-params\"\n",
    "                    + \"-noise\"\n",
    "                    + str(params[\"noise_level\"])\n",
    "                )\n",
    "            param_model = load_model(\n",
    "                param_model,\n",
    "                file_name,\n",
    "                params[\"disease\"],\n",
    "                params[\"county_id\"],\n",
    "                params[\"pred_week\"],\n",
    "                devices[0],\n",
    "            )\n",
    "\n",
    "        num_step = training_num_steps + DAYS_HEAD\n",
    "        batch_predictions = []\n",
    "        counties_predicted = []\n",
    "        learned_params = []\n",
    "        with torch.no_grad():\n",
    "            for batch, (counties, meta, x, y) in enumerate(train_loader):\n",
    "                # construct abm for each forward pass\n",
    "                abm = build_simulator(params, devices, counties)\n",
    "                # forward pass param model\n",
    "                meta = meta.to(devices[0])\n",
    "                x = x.to(devices[0])\n",
    "                param_values = param_model_forward(param_model, params, x, meta)\n",
    "                # forward simulator for several time steps\n",
    "                preds = forward_simulator(\n",
    "                    params, param_values, abm, num_step, counties, devices\n",
    "                )\n",
    "                batch_predictions.append(preds)\n",
    "                counties_predicted.extend(counties)\n",
    "                learned_params.extend(np.array(param_values.cpu().detach()))\n",
    "        predictions = torch.cat(batch_predictions, axis=0)\n",
    "        # we only care about the last predictions\n",
    "        # predictions are weekly, so we only care about the last 4\n",
    "        if params[\"disease\"] == \"Flu\":\n",
    "            predictions = predictions.squeeze(2)[:, -DAYS_HEAD // 7 :]\n",
    "        else:\n",
    "            predictions = predictions.squeeze(2)[:, -DAYS_HEAD:]\n",
    "        \"\"\" remove grad \"\"\"\n",
    "        predictions = predictions.cpu().detach()\n",
    "\n",
    "        \"\"\" release memory \"\"\"\n",
    "        param_model = None\n",
    "        abm = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \"\"\" plot losses \"\"\"\n",
    "        # only if trained\n",
    "        if train_flag:\n",
    "            disease = params[\"disease\"]\n",
    "            if params[\"joint\"]:\n",
    "                FIGPATH = f\"./Figures/{disease}/joint/\"\n",
    "            else:\n",
    "                county_id = params[\"county_id\"]\n",
    "                FIGPATH = f\"./Figures/{disease}/{county_id}/\"\n",
    "            if not os.path.exists(FIGPATH):\n",
    "                os.makedirs(FIGPATH)\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "            ax.plot(losses)\n",
    "            pred_week = params[\"pred_week\"]\n",
    "            fig.savefig(FIGPATH + f\"/losses_{pred_week}.png\")\n",
    "        print(\"-\" * 60)\n",
    "        return counties_predicted, np.array(predictions), learned_params\n",
    "\n",
    "\n",
    "def train_predict(args):\n",
    "    # Setting seed\n",
    "    print(\"=\" * 60)\n",
    "    if args.joint:\n",
    "        print(f\"state {args.state} week {args.pred_week}\")\n",
    "    else:\n",
    "        print(f\"county {args.county_id} week {args.pred_week}\")\n",
    "    print(\"Seed used for python random, numpy and torch is {}\".format(args.seed))\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    params = {}\n",
    "    params[\"seed\"] = args.seed\n",
    "    params[\"num_runs\"] = args.num_runs\n",
    "    params[\"disease\"] = args.disease\n",
    "    params[\"pred_week\"] = args.pred_week\n",
    "    params[\"joint\"] = args.joint\n",
    "    params[\"inference_only\"] = args.inference_only\n",
    "    params[\"noise_level\"] = args.noise  # for robustness experiments\n",
    "    # state\n",
    "    params[\"state\"] = args.state\n",
    "    if params[\"joint\"]:\n",
    "        # verify it is a state\n",
    "        assert params[\"state\"] in states\n",
    "    else:\n",
    "        params[\"county_id\"] = args.county_id\n",
    "        # verify county belong to state\n",
    "        assert params[\"county_id\"] in counties[params[\"state\"]]\n",
    "    params[\"model_name\"] = args.model_name\n",
    "\n",
    "    if args.dev == [\"cpu\"]:\n",
    "        devices = [torch.device(\"cpu\")]\n",
    "    else:\n",
    "        devices = [torch.device(f\"cuda:{i}\") for i in args.dev]\n",
    "\n",
    "    print(\"devices used:\", devices)\n",
    "    verbose = False\n",
    "    counties_predicted, predictions, learned_params = runner(params, devices, verbose)\n",
    "\n",
    "    return counties_predicted, predictions, learned_params\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_predictions(\n",
    "    disease: str,\n",
    "    model_name: str,\n",
    "    region: str,\n",
    "    pred_week: str,\n",
    "    death_predictions: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given an array w/ predictions, save as csv\n",
    "    \"\"\"\n",
    "    data = np.array([np.arange(len(death_predictions)) + 1, death_predictions])\n",
    "    if disease == \"COVID\":\n",
    "        df = pd.DataFrame(data.transpose(), columns=[\"k_ahead\", \"deaths\"])\n",
    "    elif disease == \"Flu\":\n",
    "        df = pd.DataFrame(data.transpose(), columns=[\"k_ahead\", \"ili\"])\n",
    "    else:\n",
    "        raise ValueError(\"disease must be COVID or Flu\")\n",
    "    df[\"k_ahead\"] = df[\"k_ahead\"].astype(\"int8\")\n",
    "    path = \"./Results/{}/{}/\".format(disease, region)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_name = \"preds_{}_{}.csv\".format(model_name, pred_week)\n",
    "    df.to_csv(path + file_name, index=False)\n",
    "\n",
    "\n",
    "def save_params(\n",
    "    disease: str,\n",
    "    model_name: str,\n",
    "    region: str,\n",
    "    pred_week: str,\n",
    "    param_values: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given an array w/ predictions, save as csv\n",
    "    \"\"\"\n",
    "\n",
    "    path = \"./Results/{}/{}/\".format(disease, region)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_name = \"params_{}_{}.csv\".format(model_name, pred_week)\n",
    "    np.savetxt(path + file_name, param_values, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parsing command line arguments\n",
    "    parser = argparse.ArgumentParser(description=\"GradABM for COVID-19 and Flu.\")\n",
    "    parser.add_argument(\"-m\", \"--model_name\", help=\"Model name.\", default=\"GradABM\")\n",
    "    parser.add_argument(\n",
    "        \"-di\", \"--disease\", help=\"Disease: COVID or Flu.\", default=\"COVID\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-s\",\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        help=\"Seed for python random, numpy and torch\",\n",
    "        default=6666,\n",
    "    )\n",
    "    parser.add_argument(\"-n\", \"--num_runs\", type=int, help=\"Number of runs\", default=1)\n",
    "    parser.add_argument(\"-st\", \"--state\", help=\"State to predict\", default=\"MA\")\n",
    "    parser.add_argument(\n",
    "        \"-c\",\n",
    "        \"--county_id\",\n",
    "        help=\"County to predict, only when not using joint training\",\n",
    "        default=\"25001\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--dev\",\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "        default=\"0\",\n",
    "        help=\"Device number to use. Put list for multiple.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-ew\",\n",
    "        \"--pred_ew\",\n",
    "        type=str,\n",
    "        default=\"202021\",\n",
    "        help=\"Prediction week in CDC format\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-j\", \"--joint\", action=\"store_true\", help=\"Train all counties jointly\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-i\",\n",
    "        \"--inference_only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Will not train if True, inference only\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-no\",\n",
    "        \"--noise\",\n",
    "        type=int,\n",
    "        help=\"Noise level for robustness experiments\",\n",
    "        default=0,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\",\n",
    "        \"--results_file_postfix\",\n",
    "        help=\"Postfix to be appended to output dir for ease of interpretation\",\n",
    "        default=\"\",\n",
    "    )\n",
    "    parser.set_defaults(joint=True)  # make true when removing no joint\n",
    "    parser.set_defaults(inference_only=False)  # make true when removing no joint\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # get list of epiweeks for iteration\n",
    "    disease = args.disease\n",
    "    model_name = args.model_name\n",
    "    pred_ew = Week.fromstring(args.pred_ew)\n",
    "\n",
    "    def run_all_weeks(args):\n",
    "        args.pred_week = pred_ew.cdcformat()\n",
    "        try:\n",
    "            counties_predicted, predictions, learned_params = train_predict(args)\n",
    "            num_counties = len(counties_predicted)\n",
    "            for c in range(num_counties):\n",
    "                save_predictions(\n",
    "                    disease,\n",
    "                    model_name,\n",
    "                    counties_predicted[c],\n",
    "                    str(pred_ew),\n",
    "                    predictions[c, :],\n",
    "                )\n",
    "                save_params(\n",
    "                    disease,\n",
    "                    model_name,\n",
    "                    counties_predicted[c],\n",
    "                    str(pred_ew),\n",
    "                    learned_params[c],\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"exception: did not work for {args.state} week {pred_ew}: \"\n",
    "                + str(e)\n",
    "                + \"\\n\"\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "\n",
    "    run_all_weeks(copy(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
